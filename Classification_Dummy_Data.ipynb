{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PrPBhdJocVzo"
      },
      "outputs": [],
      "source": [
        "# Penjelasan Konseptual:\n",
        "# Pada sel ini, kita akan:\n",
        "# - Mengimpor library yang diperlukan: torch untuk PyTorch, sklearn untuk membuat data dummy, dan matplotlib/numpy untuk analisis tambahan.\n",
        "# - Membuat dataset dummy menggunakan make_classification dari sklearn.\n",
        "#   Dataset ini akan memiliki sejumlah fitur tertentu dan dua kelas untuk klasifikasi binari.\n",
        "# - Setelah itu, data akan di-split menjadi data latih dan data uji.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Buat dummy data untuk klasifikasi\n",
        "# n_samples: jumlah data, n_features: jumlah fitur, n_informative: fitur yang benar-benar berpengaruh, n_redundant: fitur duplikat linear.\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=0, n_classes=2, random_state=42)\n",
        "\n",
        "# Normalisasi data input agar mempermudah training\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Pisahkan data menjadi train dan test (80%:20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Konversi data menjadi tensor PyTorch\n",
        "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_t = torch.tensor(y_test, dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Penjelasan Konseptual:\n",
        "# Disini kita membuat sebuah fungsi pembuat model MLP yang dapat mengakomodasi:\n",
        "# - Jumlah hidden layer\n",
        "# - Jumlah neuron per layer\n",
        "# - Jenis fungsi aktivasi\n",
        "#\n",
        "# Model sederhana ini akan memiliki:\n",
        "# Input Layer: sesuai dengan jumlah fitur (misalnya 10)\n",
        "# Hidden Layer: berdasarkan parameter yang diberikan (bisa 1, 2, atau 3 layer)\n",
        "# Output Layer: 2 neuron (karena klasifikasi binary)\n",
        "#\n",
        "# Fungsi aktivasi yang digunakan dapat dipilih dari: linear (tidak ada aktivasi), sigmoid, relu, tanh.\n",
        "# Softmax biasanya digunakan di output layer untuk menghasilkan probabilitas, tetapi disini kita juga akan coba di hidden layer hanya untuk demonstrasi.\n",
        "#\n",
        "# Catatan:\n",
        "# - \"Linear\" di sini berarti tidak menggunakan fungsi aktivasi sama sekali, hanya linear mapping dari input ke output.\n",
        "# - Bila menggunakan fungsi aktivasi softmax di hidden layer, secara konsep agak tidak lazim, tapi kita coba hanya untuk perbandingan eksperimental.\n",
        "\n",
        "def create_mlp_model(input_dim, hidden_layers, hidden_neurons, activation_func):\n",
        "    # Dictionary fungsi aktivasi\n",
        "    activation_dict = {\n",
        "        'linear': nn.Identity(),\n",
        "        'sigmoid': nn.Sigmoid(),\n",
        "        'relu': nn.ReLU(),\n",
        "        'tanh': nn.Tanh(),\n",
        "        # Softmax butuh dim parameter (biasanya dim=1 untuk batch wise)\n",
        "        # Jika ingin menggunakan di hidden layer, kita asumsikan dim=1.\n",
        "        'softmax': nn.Softmax(dim=1)\n",
        "    }\n",
        "\n",
        "    # List untuk menampung layer MLP secara berurutan\n",
        "    layers = []\n",
        "\n",
        "    # Input to first hidden layer\n",
        "    prev_dim = input_dim\n",
        "    for i in range(hidden_layers):\n",
        "        # Tambahkan layer linear\n",
        "        layers.append(nn.Linear(prev_dim, hidden_neurons))\n",
        "        # Tambahkan aktivasi\n",
        "        layers.append(activation_dict[activation_func])\n",
        "        prev_dim = hidden_neurons\n",
        "\n",
        "    # Output layer (2 kelas)\n",
        "    layers.append(nn.Linear(prev_dim, 2))\n",
        "\n",
        "    # Model sequential\n",
        "    model = nn.Sequential(*layers)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "RBo9efMXdplN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Penjelasan Konseptual:\n",
        "# Kita perlu fungsi untuk:\n",
        "# 1. Melatih model (training loop)\n",
        "# 2. Mengukur akurasi pada data uji\n",
        "#\n",
        "# Fungsi train_model akan:\n",
        "# - Menerima model, data latih, target latih, optimizer, loss function, dan jumlah epoch.\n",
        "# - Pada setiap epoch, melakukan forward pass, menghitung loss, backward pass (gradient), dan update parameter model.\n",
        "#\n",
        "# Fungsi evaluate akan:\n",
        "# - Menggunakan model terlatih untuk memprediksi kelas data uji.\n",
        "# - Menghitung akurasi (jumlah prediksi benar / total data uji).\n",
        "\n",
        "def train_model(model, X_train, y_train, epochs=50, lr=0.001):\n",
        "    # Optimizer Adam untuk update parameter model\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    # Loss function Cross Entropy untuk klasifikasi\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Set model ke mode training\n",
        "        model.train()\n",
        "\n",
        "        optimizer.zero_grad()        # Reset gradient\n",
        "        outputs = model(X_train)     # Forward pass\n",
        "        loss = criterion(outputs, y_train)  # Hitung loss\n",
        "        loss.backward()              # Backward pass\n",
        "        optimizer.step()             # Update parameter\n",
        "\n",
        "        # Optional: print loss setiap beberapa epoch untuk monitoring\n",
        "        if (epoch+1) % 20 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "def evaluate(model, X_test, y_test):\n",
        "    # Set model ke mode evaluasi\n",
        "    model.eval()\n",
        "    # Dengan torch.no_grad() agar tidak menghitung gradient\n",
        "    with torch.no_grad():\n",
        "        outputs = model(X_test)\n",
        "        # Prediksi kelas = argmax dari output logit\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "mgOS5MQzd1yS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Penjelasan Konseptual:\n",
        "# Kita akan bereksperimen dengan beberapa konfigurasi:\n",
        "# 1. Jumlah hidden layer: [1, 2, 3]\n",
        "# 2. Jumlah neuron di hidden layer: [4, 8, 16, 32, 64]\n",
        "# 3. Fungsi aktivasi: ['linear', 'sigmoid', 'relu', 'tanh', 'softmax']\n",
        "#\n",
        "# Kita akan melatih setiap kombinasi secara singkat (misalnya 100 epoch) dan menghitung akurasinya.\n",
        "# Peringatan: Akan ada banyak kombinasi, mungkin butuh waktu. Sebagai contoh, kita batasi saja sebagian kombinasi atau gunakan epoch lebih sedikit.\n",
        "#\n",
        "# Output: Kita akan menyimpan hasil akurasi dalam sebuah list untuk analisis.\n",
        "#\n",
        "# Catatan:\n",
        "# - Dalam praktik, sebaiknya jumlah kombinasi tidak terlalu banyak agar tidak memakan banyak waktu.\n",
        "# - Disini kita hanya mendemokan konsep.\n",
        "\n",
        "hidden_layers_list = [1, 2, 3]\n",
        "hidden_neurons_list = [4, 16, 64]   # Contoh subset saja agar cepat\n",
        "activation_funcs = ['linear', 'sigmoid', 'relu', 'tanh', 'softmax']\n",
        "\n",
        "results = []\n",
        "\n",
        "for hl in hidden_layers_list:\n",
        "    for hn in hidden_neurons_list:\n",
        "        for af in activation_funcs:\n",
        "            print(f\"\\nTraining model with {hl} hidden layer(s), {hn} neurons, activation={af}\")\n",
        "\n",
        "            # Buat model baru\n",
        "            model = create_mlp_model(input_dim=10, hidden_layers=hl, hidden_neurons=hn, activation_func=af)\n",
        "\n",
        "            # Latih model\n",
        "            train_model(model, X_train_t, y_train_t, epochs=100, lr=0.01)\n",
        "\n",
        "            # Evaluasi model\n",
        "            acc = evaluate(model, X_test_t, y_test_t)\n",
        "            print(f\"Accuracy on test: {acc:.4f}\")\n",
        "\n",
        "            # Simpan hasil\n",
        "            results.append((hl, hn, af, acc))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Khp_6f7d5gM",
        "outputId": "30daa4a6-a13b-4b8a-f6cf-cdb8c777be7e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model with 1 hidden layer(s), 4 neurons, activation=linear\n",
            "Epoch [20/100], Loss: 0.5676\n",
            "Epoch [40/100], Loss: 0.4291\n",
            "Epoch [60/100], Loss: 0.4063\n",
            "Epoch [80/100], Loss: 0.4057\n",
            "Epoch [100/100], Loss: 0.4055\n",
            "Accuracy on test: 0.7700\n",
            "\n",
            "Training model with 1 hidden layer(s), 4 neurons, activation=sigmoid\n",
            "Epoch [20/100], Loss: 0.6883\n",
            "Epoch [40/100], Loss: 0.6527\n",
            "Epoch [60/100], Loss: 0.5804\n",
            "Epoch [80/100], Loss: 0.4924\n",
            "Epoch [100/100], Loss: 0.4285\n",
            "Accuracy on test: 0.7800\n",
            "\n",
            "Training model with 1 hidden layer(s), 4 neurons, activation=relu\n",
            "Epoch [20/100], Loss: 0.5595\n",
            "Epoch [40/100], Loss: 0.3825\n",
            "Epoch [60/100], Loss: 0.2960\n",
            "Epoch [80/100], Loss: 0.2591\n",
            "Epoch [100/100], Loss: 0.2340\n",
            "Accuracy on test: 0.8600\n",
            "\n",
            "Training model with 1 hidden layer(s), 4 neurons, activation=tanh\n",
            "Epoch [20/100], Loss: 0.5803\n",
            "Epoch [40/100], Loss: 0.4283\n",
            "Epoch [60/100], Loss: 0.3842\n",
            "Epoch [80/100], Loss: 0.3382\n",
            "Epoch [100/100], Loss: 0.3001\n",
            "Accuracy on test: 0.8000\n",
            "\n",
            "Training model with 1 hidden layer(s), 4 neurons, activation=softmax\n",
            "Epoch [20/100], Loss: 0.6482\n",
            "Epoch [40/100], Loss: 0.5380\n",
            "Epoch [60/100], Loss: 0.4289\n",
            "Epoch [80/100], Loss: 0.3571\n",
            "Epoch [100/100], Loss: 0.3125\n",
            "Accuracy on test: 0.8200\n",
            "\n",
            "Training model with 1 hidden layer(s), 16 neurons, activation=linear\n",
            "Epoch [20/100], Loss: 0.4130\n",
            "Epoch [40/100], Loss: 0.4086\n",
            "Epoch [60/100], Loss: 0.4059\n",
            "Epoch [80/100], Loss: 0.4055\n",
            "Epoch [100/100], Loss: 0.4054\n",
            "Accuracy on test: 0.7700\n",
            "\n",
            "Training model with 1 hidden layer(s), 16 neurons, activation=sigmoid\n",
            "Epoch [20/100], Loss: 0.5503\n",
            "Epoch [40/100], Loss: 0.4175\n",
            "Epoch [60/100], Loss: 0.3894\n",
            "Epoch [80/100], Loss: 0.3730\n",
            "Epoch [100/100], Loss: 0.3543\n",
            "Accuracy on test: 0.7800\n",
            "\n",
            "Training model with 1 hidden layer(s), 16 neurons, activation=relu\n",
            "Epoch [20/100], Loss: 0.3922\n",
            "Epoch [40/100], Loss: 0.2658\n",
            "Epoch [60/100], Loss: 0.1941\n",
            "Epoch [80/100], Loss: 0.1513\n",
            "Epoch [100/100], Loss: 0.1258\n",
            "Accuracy on test: 0.8950\n",
            "\n",
            "Training model with 1 hidden layer(s), 16 neurons, activation=tanh\n",
            "Epoch [20/100], Loss: 0.4076\n",
            "Epoch [40/100], Loss: 0.3677\n",
            "Epoch [60/100], Loss: 0.3221\n",
            "Epoch [80/100], Loss: 0.2683\n",
            "Epoch [100/100], Loss: 0.2136\n",
            "Accuracy on test: 0.8500\n",
            "\n",
            "Training model with 1 hidden layer(s), 16 neurons, activation=softmax\n",
            "Epoch [20/100], Loss: 0.6279\n",
            "Epoch [40/100], Loss: 0.5151\n",
            "Epoch [60/100], Loss: 0.4158\n",
            "Epoch [80/100], Loss: 0.3441\n",
            "Epoch [100/100], Loss: 0.2941\n",
            "Accuracy on test: 0.8200\n",
            "\n",
            "Training model with 1 hidden layer(s), 64 neurons, activation=linear\n",
            "Epoch [20/100], Loss: 0.4191\n",
            "Epoch [40/100], Loss: 0.4062\n",
            "Epoch [60/100], Loss: 0.4055\n",
            "Epoch [80/100], Loss: 0.4055\n",
            "Epoch [100/100], Loss: 0.4054\n",
            "Accuracy on test: 0.7700\n",
            "\n",
            "Training model with 1 hidden layer(s), 64 neurons, activation=sigmoid\n",
            "Epoch [20/100], Loss: 0.5004\n",
            "Epoch [40/100], Loss: 0.4009\n",
            "Epoch [60/100], Loss: 0.3903\n",
            "Epoch [80/100], Loss: 0.3774\n",
            "Epoch [100/100], Loss: 0.3609\n",
            "Accuracy on test: 0.7800\n",
            "\n",
            "Training model with 1 hidden layer(s), 64 neurons, activation=relu\n",
            "Epoch [20/100], Loss: 0.2910\n",
            "Epoch [40/100], Loss: 0.1624\n",
            "Epoch [60/100], Loss: 0.0991\n",
            "Epoch [80/100], Loss: 0.0661\n",
            "Epoch [100/100], Loss: 0.0456\n",
            "Accuracy on test: 0.8900\n",
            "\n",
            "Training model with 1 hidden layer(s), 64 neurons, activation=tanh\n",
            "Epoch [20/100], Loss: 0.3891\n",
            "Epoch [40/100], Loss: 0.3002\n",
            "Epoch [60/100], Loss: 0.1953\n",
            "Epoch [80/100], Loss: 0.1213\n",
            "Epoch [100/100], Loss: 0.0784\n",
            "Accuracy on test: 0.9050\n",
            "\n",
            "Training model with 1 hidden layer(s), 64 neurons, activation=softmax\n",
            "Epoch [20/100], Loss: 0.6337\n",
            "Epoch [40/100], Loss: 0.5156\n",
            "Epoch [60/100], Loss: 0.4026\n",
            "Epoch [80/100], Loss: 0.3136\n",
            "Epoch [100/100], Loss: 0.2514\n",
            "Accuracy on test: 0.8550\n",
            "\n",
            "Training model with 2 hidden layer(s), 4 neurons, activation=linear\n",
            "Epoch [20/100], Loss: 0.5857\n",
            "Epoch [40/100], Loss: 0.4239\n",
            "Epoch [60/100], Loss: 0.4063\n",
            "Epoch [80/100], Loss: 0.4056\n",
            "Epoch [100/100], Loss: 0.4054\n",
            "Accuracy on test: 0.7700\n",
            "\n",
            "Training model with 2 hidden layer(s), 4 neurons, activation=sigmoid\n",
            "Epoch [20/100], Loss: 0.6877\n",
            "Epoch [40/100], Loss: 0.6605\n",
            "Epoch [60/100], Loss: 0.5577\n",
            "Epoch [80/100], Loss: 0.4223\n",
            "Epoch [100/100], Loss: 0.3730\n",
            "Accuracy on test: 0.7800\n",
            "\n",
            "Training model with 2 hidden layer(s), 4 neurons, activation=relu\n",
            "Epoch [20/100], Loss: 0.5554\n",
            "Epoch [40/100], Loss: 0.3793\n",
            "Epoch [60/100], Loss: 0.3173\n",
            "Epoch [80/100], Loss: 0.2668\n",
            "Epoch [100/100], Loss: 0.2354\n",
            "Accuracy on test: 0.8700\n",
            "\n",
            "Training model with 2 hidden layer(s), 4 neurons, activation=tanh\n",
            "Epoch [20/100], Loss: 0.6808\n",
            "Epoch [40/100], Loss: 0.5230\n",
            "Epoch [60/100], Loss: 0.3882\n",
            "Epoch [80/100], Loss: 0.3729\n",
            "Epoch [100/100], Loss: 0.3618\n",
            "Accuracy on test: 0.7800\n",
            "\n",
            "Training model with 2 hidden layer(s), 4 neurons, activation=softmax\n",
            "Epoch [20/100], Loss: 0.6880\n",
            "Epoch [40/100], Loss: 0.6655\n",
            "Epoch [60/100], Loss: 0.5810\n",
            "Epoch [80/100], Loss: 0.4459\n",
            "Epoch [100/100], Loss: 0.3464\n",
            "Accuracy on test: 0.8250\n",
            "\n",
            "Training model with 2 hidden layer(s), 16 neurons, activation=linear\n",
            "Epoch [20/100], Loss: 0.4314\n",
            "Epoch [40/100], Loss: 0.4065\n",
            "Epoch [60/100], Loss: 0.4057\n",
            "Epoch [80/100], Loss: 0.4054\n",
            "Epoch [100/100], Loss: 0.4054\n",
            "Accuracy on test: 0.7700\n",
            "\n",
            "Training model with 2 hidden layer(s), 16 neurons, activation=sigmoid\n",
            "Epoch [20/100], Loss: 0.6481\n",
            "Epoch [40/100], Loss: 0.4059\n",
            "Epoch [60/100], Loss: 0.3695\n",
            "Epoch [80/100], Loss: 0.3564\n",
            "Epoch [100/100], Loss: 0.3409\n",
            "Accuracy on test: 0.7900\n",
            "\n",
            "Training model with 2 hidden layer(s), 16 neurons, activation=relu\n",
            "Epoch [20/100], Loss: 0.3538\n",
            "Epoch [40/100], Loss: 0.1772\n",
            "Epoch [60/100], Loss: 0.1081\n",
            "Epoch [80/100], Loss: 0.0704\n",
            "Epoch [100/100], Loss: 0.0444\n",
            "Accuracy on test: 0.8900\n",
            "\n",
            "Training model with 2 hidden layer(s), 16 neurons, activation=tanh\n",
            "Epoch [20/100], Loss: 0.4007\n",
            "Epoch [40/100], Loss: 0.3449\n",
            "Epoch [60/100], Loss: 0.2509\n",
            "Epoch [80/100], Loss: 0.1572\n",
            "Epoch [100/100], Loss: 0.0902\n",
            "Accuracy on test: 0.8950\n",
            "\n",
            "Training model with 2 hidden layer(s), 16 neurons, activation=softmax\n",
            "Epoch [20/100], Loss: 0.6900\n",
            "Epoch [40/100], Loss: 0.6475\n",
            "Epoch [60/100], Loss: 0.5155\n",
            "Epoch [80/100], Loss: 0.3775\n",
            "Epoch [100/100], Loss: 0.2834\n",
            "Accuracy on test: 0.8300\n",
            "\n",
            "Training model with 2 hidden layer(s), 64 neurons, activation=linear\n",
            "Epoch [20/100], Loss: 0.4065\n",
            "Epoch [40/100], Loss: 0.4065\n",
            "Epoch [60/100], Loss: 0.4055\n",
            "Epoch [80/100], Loss: 0.4054\n",
            "Epoch [100/100], Loss: 0.4054\n",
            "Accuracy on test: 0.7700\n",
            "\n",
            "Training model with 2 hidden layer(s), 64 neurons, activation=sigmoid\n",
            "Epoch [20/100], Loss: 0.5274\n",
            "Epoch [40/100], Loss: 0.3918\n",
            "Epoch [60/100], Loss: 0.3617\n",
            "Epoch [80/100], Loss: 0.3352\n",
            "Epoch [100/100], Loss: 0.2867\n",
            "Accuracy on test: 0.7950\n",
            "\n",
            "Training model with 2 hidden layer(s), 64 neurons, activation=relu\n",
            "Epoch [20/100], Loss: 0.1671\n",
            "Epoch [40/100], Loss: 0.0367\n",
            "Epoch [60/100], Loss: 0.0044\n",
            "Epoch [80/100], Loss: 0.0011\n",
            "Epoch [100/100], Loss: 0.0005\n",
            "Accuracy on test: 0.8800\n",
            "\n",
            "Training model with 2 hidden layer(s), 64 neurons, activation=tanh\n",
            "Epoch [20/100], Loss: 0.3041\n",
            "Epoch [40/100], Loss: 0.1159\n",
            "Epoch [60/100], Loss: 0.0394\n",
            "Epoch [80/100], Loss: 0.0117\n",
            "Epoch [100/100], Loss: 0.0043\n",
            "Accuracy on test: 0.9200\n",
            "\n",
            "Training model with 2 hidden layer(s), 64 neurons, activation=softmax\n",
            "Epoch [20/100], Loss: 0.6853\n",
            "Epoch [40/100], Loss: 0.6138\n",
            "Epoch [60/100], Loss: 0.4657\n",
            "Epoch [80/100], Loss: 0.3244\n",
            "Epoch [100/100], Loss: 0.2358\n",
            "Accuracy on test: 0.8450\n",
            "\n",
            "Training model with 3 hidden layer(s), 4 neurons, activation=linear\n",
            "Epoch [20/100], Loss: 0.4318\n",
            "Epoch [40/100], Loss: 0.4071\n",
            "Epoch [60/100], Loss: 0.4058\n",
            "Epoch [80/100], Loss: 0.4055\n",
            "Epoch [100/100], Loss: 0.4054\n",
            "Accuracy on test: 0.7700\n",
            "\n",
            "Training model with 3 hidden layer(s), 4 neurons, activation=sigmoid\n",
            "Epoch [20/100], Loss: 0.6928\n",
            "Epoch [40/100], Loss: 0.6899\n",
            "Epoch [60/100], Loss: 0.6646\n",
            "Epoch [80/100], Loss: 0.5405\n",
            "Epoch [100/100], Loss: 0.3900\n",
            "Accuracy on test: 0.7800\n",
            "\n",
            "Training model with 3 hidden layer(s), 4 neurons, activation=relu\n",
            "Epoch [20/100], Loss: 0.6338\n",
            "Epoch [40/100], Loss: 0.4976\n",
            "Epoch [60/100], Loss: 0.4193\n",
            "Epoch [80/100], Loss: 0.3762\n",
            "Epoch [100/100], Loss: 0.3509\n",
            "Accuracy on test: 0.7800\n",
            "\n",
            "Training model with 3 hidden layer(s), 4 neurons, activation=tanh\n",
            "Epoch [20/100], Loss: 0.5973\n",
            "Epoch [40/100], Loss: 0.3843\n",
            "Epoch [60/100], Loss: 0.3221\n",
            "Epoch [80/100], Loss: 0.2751\n",
            "Epoch [100/100], Loss: 0.2250\n",
            "Accuracy on test: 0.8450\n",
            "\n",
            "Training model with 3 hidden layer(s), 4 neurons, activation=softmax\n",
            "Epoch [20/100], Loss: 0.6877\n",
            "Epoch [40/100], Loss: 0.6296\n",
            "Epoch [60/100], Loss: 0.4720\n",
            "Epoch [80/100], Loss: 0.3439\n",
            "Epoch [100/100], Loss: 0.2856\n",
            "Accuracy on test: 0.8300\n",
            "\n",
            "Training model with 3 hidden layer(s), 16 neurons, activation=linear\n",
            "Epoch [20/100], Loss: 0.4126\n",
            "Epoch [40/100], Loss: 0.4059\n",
            "Epoch [60/100], Loss: 0.4056\n",
            "Epoch [80/100], Loss: 0.4054\n",
            "Epoch [100/100], Loss: 0.4054\n",
            "Accuracy on test: 0.7700\n",
            "\n",
            "Training model with 3 hidden layer(s), 16 neurons, activation=sigmoid\n",
            "Epoch [20/100], Loss: 0.6878\n",
            "Epoch [40/100], Loss: 0.5026\n",
            "Epoch [60/100], Loss: 0.3648\n",
            "Epoch [80/100], Loss: 0.3513\n",
            "Epoch [100/100], Loss: 0.3330\n",
            "Accuracy on test: 0.7800\n",
            "\n",
            "Training model with 3 hidden layer(s), 16 neurons, activation=relu\n",
            "Epoch [20/100], Loss: 0.2679\n",
            "Epoch [40/100], Loss: 0.1550\n",
            "Epoch [60/100], Loss: 0.0889\n",
            "Epoch [80/100], Loss: 0.0437\n",
            "Epoch [100/100], Loss: 0.0213\n",
            "Accuracy on test: 0.9050\n",
            "\n",
            "Training model with 3 hidden layer(s), 16 neurons, activation=tanh\n",
            "Epoch [20/100], Loss: 0.3882\n",
            "Epoch [40/100], Loss: 0.2405\n",
            "Epoch [60/100], Loss: 0.1722\n",
            "Epoch [80/100], Loss: 0.0911\n",
            "Epoch [100/100], Loss: 0.0354\n",
            "Accuracy on test: 0.8900\n",
            "\n",
            "Training model with 3 hidden layer(s), 16 neurons, activation=softmax\n",
            "Epoch [20/100], Loss: 0.6929\n",
            "Epoch [40/100], Loss: 0.6840\n",
            "Epoch [60/100], Loss: 0.5968\n",
            "Epoch [80/100], Loss: 0.4125\n",
            "Epoch [100/100], Loss: 0.2866\n",
            "Accuracy on test: 0.8400\n",
            "\n",
            "Training model with 3 hidden layer(s), 64 neurons, activation=linear\n",
            "Epoch [20/100], Loss: 0.4096\n",
            "Epoch [40/100], Loss: 0.4061\n",
            "Epoch [60/100], Loss: 0.4055\n",
            "Epoch [80/100], Loss: 0.4054\n",
            "Epoch [100/100], Loss: 0.4054\n",
            "Accuracy on test: 0.7700\n",
            "\n",
            "Training model with 3 hidden layer(s), 64 neurons, activation=sigmoid\n",
            "Epoch [20/100], Loss: 0.6897\n",
            "Epoch [40/100], Loss: 0.3991\n",
            "Epoch [60/100], Loss: 0.3602\n",
            "Epoch [80/100], Loss: 0.3452\n",
            "Epoch [100/100], Loss: 0.3005\n",
            "Accuracy on test: 0.8000\n",
            "\n",
            "Training model with 3 hidden layer(s), 64 neurons, activation=relu\n",
            "Epoch [20/100], Loss: 0.1276\n",
            "Epoch [40/100], Loss: 0.0049\n",
            "Epoch [60/100], Loss: 0.0001\n",
            "Epoch [80/100], Loss: 0.0000\n",
            "Epoch [100/100], Loss: 0.0000\n",
            "Accuracy on test: 0.8950\n",
            "\n",
            "Training model with 3 hidden layer(s), 64 neurons, activation=tanh\n",
            "Epoch [20/100], Loss: 0.2422\n",
            "Epoch [40/100], Loss: 0.0688\n",
            "Epoch [60/100], Loss: 0.0061\n",
            "Epoch [80/100], Loss: 0.0006\n",
            "Epoch [100/100], Loss: 0.0003\n",
            "Accuracy on test: 0.9100\n",
            "\n",
            "Training model with 3 hidden layer(s), 64 neurons, activation=softmax\n",
            "Epoch [20/100], Loss: 0.6930\n",
            "Epoch [40/100], Loss: 0.6799\n",
            "Epoch [60/100], Loss: 0.5729\n",
            "Epoch [80/100], Loss: 0.3790\n",
            "Epoch [100/100], Loss: 0.2449\n",
            "Accuracy on test: 0.8450\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Penjelasan Konseptual:\n",
        "# Kita telah mengumpulkan hasil akurasi untuk berbagai kombinasi.\n",
        "# Sekarang kita bisa melihat kombinasi mana yang menghasilkan akurasi terbaik.\n",
        "#\n",
        "# Pada sel ini, kita akan mencetak kembali hasil dengan sorting berdasarkan akurasi.\n",
        "\n",
        "# Urutkan berdasarkan akurasi descending\n",
        "sorted_results = sorted(results, key=lambda x: x[3], reverse=True)\n",
        "\n",
        "print(\"Top 5 best configurations:\")\n",
        "for i in range(5):\n",
        "    hl, hn, af, acc = sorted_results[i]\n",
        "    print(f\"Hidden Layers: {hl}, Neurons: {hn}, Activation: {af}, Accuracy: {acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lOAJB7-eJFS",
        "outputId": "2db841a0-8e96-463a-f8ab-8a1f4bf2ca1e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 best configurations:\n",
            "Hidden Layers: 2, Neurons: 64, Activation: tanh, Accuracy: 0.9200\n",
            "Hidden Layers: 3, Neurons: 64, Activation: tanh, Accuracy: 0.9100\n",
            "Hidden Layers: 1, Neurons: 64, Activation: tanh, Accuracy: 0.9050\n",
            "Hidden Layers: 3, Neurons: 16, Activation: relu, Accuracy: 0.9050\n",
            "Hidden Layers: 1, Neurons: 16, Activation: relu, Accuracy: 0.8950\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Penjelasan Konseptual:\n",
        "# Pada sel ini, kita akan menyiapkan DataLoader untuk data training karena kita akan membandingkan variasi batch size.\n",
        "# DataLoader akan mengambil X_train_t dan y_train_t kemudian membuat batch sesuai ukuran batch size yang kita tentukan.\n",
        "# Kita belum jalankan training di sini; hanya menyiapkan fungsi pembantu terlebih dahulu.\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Buat dataset PyTorch\n",
        "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
        "# DataLoader akan diinisialisasi nanti di dalam loop untuk setiap batch_size."
      ],
      "metadata": {
        "id": "D5YrwJULfA4c"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Penjelasan Konseptual:\n",
        "# Fungsi train_model sebelumnya menggunakan keseluruhan data sekaligus.\n",
        "# Sekarang kita modifikasi agar fungsi tersebut menggunakan DataLoader dengan batch_size yang bervariasi.\n",
        "#\n",
        "# Argumen tambahan:\n",
        "# - batch_size: ukuran batch yang ingin digunakan.\n",
        "#\n",
        "# Proses:\n",
        "# - Set DataLoader dengan batch_size yang diberikan.\n",
        "# - Pada setiap epoch, kita loop melalui semua batch, melakukan forward, backward, update parameter.\n",
        "#\n",
        "# Fungsi evaluate tidak perlu diubah karena evaluasi tetap menggunakan keseluruhan data test secara sekaligus (bisa juga di-batch, tapi tidak masalah untuk data test yang relatif kecil).\n",
        "\n",
        "def train_model_with_batch(model, X_train, y_train, epochs=50, lr=0.001, batch_size=32):\n",
        "    # Buat DataLoader untuk training\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Optional: print loss rata-rata per epoch\n",
        "        if (epoch+1) % 20 == 0:\n",
        "            avg_loss = running_loss / len(train_loader)\n",
        "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "PynLpuVrfEGK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Penjelasan Konseptual:\n",
        "# Pada sel ini kita akan melakukan eksperimen komparasi:\n",
        "# - Epoch: [1, 10, 25, 50, 100, 250]\n",
        "# - Learning Rate: [10, 1, 0.1, 0.01, 0.001, 0.0001]\n",
        "# - Batch Size: [16, 32, 64, 128, 256, 512]\n",
        "#\n",
        "# Untuk menjaga waktu eksekusi, kita hanya akan mengambil satu konfigurasi arsitektur MLP,\n",
        "# misalnya: 1 hidden layer, 16 neuron, activation='relu'.\n",
        "#\n",
        "# Kita akan iterasi melalui semua kombinasi hyperparameter tersebut. Setiap kombinasi:\n",
        "# - Buat model baru\n",
        "# - Latih model dengan parameter tsb\n",
        "# - Evaluasi model pada test set\n",
        "# - Simpan hasilnya\n",
        "#\n",
        "# Karena jumlah kombinasi besar, ini bisa memakan waktu. Disarankan untuk mencoba subset terlebih dahulu.\n",
        "# Tetapi disini kita tampilkan kodenya secara lengkap.\n",
        "\n",
        "epoch_list = [1, 10, 25, 50, 100, 250]\n",
        "lr_list = [10, 1, 0.1, 0.01, 0.001, 0.0001]\n",
        "batch_size_list = [16, 32, 64, 128, 256, 512]\n",
        "\n",
        "# Untuk eksperimen ini, arsitektur tetap\n",
        "hidden_layers = 1\n",
        "hidden_neurons = 16\n",
        "activation = 'relu'\n",
        "\n",
        "hyperparam_results = []\n",
        "\n",
        "for ep in epoch_list:\n",
        "    for lr in lr_list:\n",
        "        for bs in batch_size_list:\n",
        "            print(f\"\\nTraining model - Epoch: {ep}, LR: {lr}, Batch_Size: {bs}\")\n",
        "            model = create_mlp_model(input_dim=10,\n",
        "                                     hidden_layers=hidden_layers,\n",
        "                                     hidden_neurons=hidden_neurons,\n",
        "                                     activation_func=activation)\n",
        "\n",
        "            # Train model dengan parameter yang ditentukan\n",
        "            train_model_with_batch(model, X_train_t, y_train_t, epochs=ep, lr=lr, batch_size=bs)\n",
        "\n",
        "            # Evaluasi\n",
        "            acc = evaluate(model, X_test_t, y_test_t)\n",
        "            print(f\"Accuracy on test: {acc:.4f}\")\n",
        "\n",
        "            # Simpan hasil\n",
        "            hyperparam_results.append((ep, lr, bs, acc))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUu9as8rfGaG",
        "outputId": "76550161-1477-4b1f-f883-e7fc459b4f6a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model - Epoch: 1, LR: 10, Batch_Size: 16\n",
            "Accuracy on test: 0.7550\n",
            "\n",
            "Training model - Epoch: 1, LR: 10, Batch_Size: 32\n",
            "Accuracy on test: 0.7550\n",
            "\n",
            "Training model - Epoch: 1, LR: 10, Batch_Size: 64\n",
            "Accuracy on test: 0.7350\n",
            "\n",
            "Training model - Epoch: 1, LR: 10, Batch_Size: 128\n",
            "Accuracy on test: 0.7550\n",
            "\n",
            "Training model - Epoch: 1, LR: 10, Batch_Size: 256\n",
            "Accuracy on test: 0.7700\n",
            "\n",
            "Training model - Epoch: 1, LR: 10, Batch_Size: 512\n",
            "Accuracy on test: 0.6000\n",
            "\n",
            "Training model - Epoch: 1, LR: 1, Batch_Size: 16\n",
            "Accuracy on test: 0.8100\n",
            "\n",
            "Training model - Epoch: 1, LR: 1, Batch_Size: 32\n",
            "Accuracy on test: 0.7650\n",
            "\n",
            "Training model - Epoch: 1, LR: 1, Batch_Size: 64\n",
            "Accuracy on test: 0.7650\n",
            "\n",
            "Training model - Epoch: 1, LR: 1, Batch_Size: 128\n",
            "Accuracy on test: 0.7750\n",
            "\n",
            "Training model - Epoch: 1, LR: 1, Batch_Size: 256\n",
            "Accuracy on test: 0.7100\n",
            "\n",
            "Training model - Epoch: 1, LR: 1, Batch_Size: 512\n",
            "Accuracy on test: 0.6450\n",
            "\n",
            "Training model - Epoch: 1, LR: 0.1, Batch_Size: 16\n",
            "Accuracy on test: 0.8450\n",
            "\n",
            "Training model - Epoch: 1, LR: 0.1, Batch_Size: 32\n",
            "Accuracy on test: 0.8050\n",
            "\n",
            "Training model - Epoch: 1, LR: 0.1, Batch_Size: 64\n",
            "Accuracy on test: 0.8350\n",
            "\n",
            "Training model - Epoch: 1, LR: 0.1, Batch_Size: 128\n",
            "Accuracy on test: 0.7850\n",
            "\n",
            "Training model - Epoch: 1, LR: 0.1, Batch_Size: 256\n",
            "Accuracy on test: 0.7550\n",
            "\n",
            "Training model - Epoch: 1, LR: 0.1, Batch_Size: 512\n",
            "Accuracy on test: 0.7550\n",
            "\n",
            "Training model - Epoch: 1, LR: 0.01, Batch_Size: 16\n",
            "Accuracy on test: 0.7950\n",
            "\n",
            "Training model - Epoch: 1, LR: 0.01, Batch_Size: 32\n",
            "Accuracy on test: 0.7700\n",
            "\n",
            "Training model - Epoch: 1, LR: 0.01, Batch_Size: 64\n",
            "Accuracy on test: 0.6200\n",
            "\n",
            "Training model - Epoch: 1, LR: 0.01, Batch_Size: 128\n",
            "Accuracy on test: 0.6550\n",
            "\n",
            "Training model - Epoch: 1, LR: 0.01, Batch_Size: 256\n",
            "Accuracy on test: 0.4850\n",
            "\n",
            "Training model - Epoch: 1, LR: 0.01, Batch_Size: 512\n",
            "Accuracy on test: 0.5550\n",
            "\n",
            "Training model - Epoch: 1, LR: 0.001, Batch_Size: 16\n",
            "Accuracy on test: 0.5950\n",
            "\n",
            "Training model - Epoch: 1, LR: 0.001, Batch_Size: 32\n",
            "Accuracy on test: 0.5700\n",
            "\n",
            "Training model - Epoch: 1, LR: 0.001, Batch_Size: 64\n",
            "Accuracy on test: 0.6900\n",
            "\n",
            "Training model - Epoch: 1, LR: 0.001, Batch_Size: 128\n",
            "Accuracy on test: 0.5250\n",
            "\n",
            "Training model - Epoch: 1, LR: 0.001, Batch_Size: 256\n",
            "Accuracy on test: 0.4700\n",
            "\n",
            "Training model - Epoch: 1, LR: 0.001, Batch_Size: 512\n",
            "Accuracy on test: 0.4950\n",
            "\n",
            "Training model - Epoch: 1, LR: 0.0001, Batch_Size: 16\n",
            "Accuracy on test: 0.3900\n",
            "\n",
            "Training model - Epoch: 1, LR: 0.0001, Batch_Size: 32\n",
            "Accuracy on test: 0.4350\n",
            "\n",
            "Training model - Epoch: 1, LR: 0.0001, Batch_Size: 64\n",
            "Accuracy on test: 0.5200\n",
            "\n",
            "Training model - Epoch: 1, LR: 0.0001, Batch_Size: 128\n",
            "Accuracy on test: 0.6100\n",
            "\n",
            "Training model - Epoch: 1, LR: 0.0001, Batch_Size: 256\n",
            "Accuracy on test: 0.3900\n",
            "\n",
            "Training model - Epoch: 1, LR: 0.0001, Batch_Size: 512\n",
            "Accuracy on test: 0.4650\n",
            "\n",
            "Training model - Epoch: 10, LR: 10, Batch_Size: 16\n",
            "Accuracy on test: 0.6200\n",
            "\n",
            "Training model - Epoch: 10, LR: 10, Batch_Size: 32\n",
            "Accuracy on test: 0.6900\n",
            "\n",
            "Training model - Epoch: 10, LR: 10, Batch_Size: 64\n",
            "Accuracy on test: 0.6600\n",
            "\n",
            "Training model - Epoch: 10, LR: 10, Batch_Size: 128\n",
            "Accuracy on test: 0.7750\n",
            "\n",
            "Training model - Epoch: 10, LR: 10, Batch_Size: 256\n",
            "Accuracy on test: 0.7900\n",
            "\n",
            "Training model - Epoch: 10, LR: 10, Batch_Size: 512\n",
            "Accuracy on test: 0.8050\n",
            "\n",
            "Training model - Epoch: 10, LR: 1, Batch_Size: 16\n",
            "Accuracy on test: 0.7250\n",
            "\n",
            "Training model - Epoch: 10, LR: 1, Batch_Size: 32\n",
            "Accuracy on test: 0.7350\n",
            "\n",
            "Training model - Epoch: 10, LR: 1, Batch_Size: 64\n",
            "Accuracy on test: 0.7850\n",
            "\n",
            "Training model - Epoch: 10, LR: 1, Batch_Size: 128\n",
            "Accuracy on test: 0.7950\n",
            "\n",
            "Training model - Epoch: 10, LR: 1, Batch_Size: 256\n",
            "Accuracy on test: 0.8000\n",
            "\n",
            "Training model - Epoch: 10, LR: 1, Batch_Size: 512\n",
            "Accuracy on test: 0.8050\n",
            "\n",
            "Training model - Epoch: 10, LR: 0.1, Batch_Size: 16\n",
            "Accuracy on test: 0.8800\n",
            "\n",
            "Training model - Epoch: 10, LR: 0.1, Batch_Size: 32\n",
            "Accuracy on test: 0.8850\n",
            "\n",
            "Training model - Epoch: 10, LR: 0.1, Batch_Size: 64\n",
            "Accuracy on test: 0.8900\n",
            "\n",
            "Training model - Epoch: 10, LR: 0.1, Batch_Size: 128\n",
            "Accuracy on test: 0.8950\n",
            "\n",
            "Training model - Epoch: 10, LR: 0.1, Batch_Size: 256\n",
            "Accuracy on test: 0.8850\n",
            "\n",
            "Training model - Epoch: 10, LR: 0.1, Batch_Size: 512\n",
            "Accuracy on test: 0.8800\n",
            "\n",
            "Training model - Epoch: 10, LR: 0.01, Batch_Size: 16\n",
            "Accuracy on test: 0.8950\n",
            "\n",
            "Training model - Epoch: 10, LR: 0.01, Batch_Size: 32\n",
            "Accuracy on test: 0.8950\n",
            "\n",
            "Training model - Epoch: 10, LR: 0.01, Batch_Size: 64\n",
            "Accuracy on test: 0.9000\n",
            "\n",
            "Training model - Epoch: 10, LR: 0.01, Batch_Size: 128\n",
            "Accuracy on test: 0.8350\n",
            "\n",
            "Training model - Epoch: 10, LR: 0.01, Batch_Size: 256\n",
            "Accuracy on test: 0.8150\n",
            "\n",
            "Training model - Epoch: 10, LR: 0.01, Batch_Size: 512\n",
            "Accuracy on test: 0.7900\n",
            "\n",
            "Training model - Epoch: 10, LR: 0.001, Batch_Size: 16\n",
            "Accuracy on test: 0.8550\n",
            "\n",
            "Training model - Epoch: 10, LR: 0.001, Batch_Size: 32\n",
            "Accuracy on test: 0.7950\n",
            "\n",
            "Training model - Epoch: 10, LR: 0.001, Batch_Size: 64\n",
            "Accuracy on test: 0.7850\n",
            "\n",
            "Training model - Epoch: 10, LR: 0.001, Batch_Size: 128\n",
            "Accuracy on test: 0.7000\n",
            "\n",
            "Training model - Epoch: 10, LR: 0.001, Batch_Size: 256\n",
            "Accuracy on test: 0.5900\n",
            "\n",
            "Training model - Epoch: 10, LR: 0.001, Batch_Size: 512\n",
            "Accuracy on test: 0.5700\n",
            "\n",
            "Training model - Epoch: 10, LR: 0.0001, Batch_Size: 16\n",
            "Accuracy on test: 0.5100\n",
            "\n",
            "Training model - Epoch: 10, LR: 0.0001, Batch_Size: 32\n",
            "Accuracy on test: 0.4800\n",
            "\n",
            "Training model - Epoch: 10, LR: 0.0001, Batch_Size: 64\n",
            "Accuracy on test: 0.5150\n",
            "\n",
            "Training model - Epoch: 10, LR: 0.0001, Batch_Size: 128\n",
            "Accuracy on test: 0.5550\n",
            "\n",
            "Training model - Epoch: 10, LR: 0.0001, Batch_Size: 256\n",
            "Accuracy on test: 0.5350\n",
            "\n",
            "Training model - Epoch: 10, LR: 0.0001, Batch_Size: 512\n",
            "Accuracy on test: 0.5500\n",
            "\n",
            "Training model - Epoch: 25, LR: 10, Batch_Size: 16\n",
            "Epoch [20/25], Loss: 0.7678\n",
            "Accuracy on test: 0.6600\n",
            "\n",
            "Training model - Epoch: 25, LR: 10, Batch_Size: 32\n",
            "Epoch [20/25], Loss: 0.6046\n",
            "Accuracy on test: 0.6750\n",
            "\n",
            "Training model - Epoch: 25, LR: 10, Batch_Size: 64\n",
            "Epoch [20/25], Loss: 0.7643\n",
            "Accuracy on test: 0.7100\n",
            "\n",
            "Training model - Epoch: 25, LR: 10, Batch_Size: 128\n",
            "Epoch [20/25], Loss: 0.9761\n",
            "Accuracy on test: 0.6850\n",
            "\n",
            "Training model - Epoch: 25, LR: 10, Batch_Size: 256\n",
            "Epoch [20/25], Loss: 3.4554\n",
            "Accuracy on test: 0.7750\n",
            "\n",
            "Training model - Epoch: 25, LR: 10, Batch_Size: 512\n",
            "Epoch [20/25], Loss: 8.6773\n",
            "Accuracy on test: 0.7850\n",
            "\n",
            "Training model - Epoch: 25, LR: 1, Batch_Size: 16\n",
            "Epoch [20/25], Loss: 0.5694\n",
            "Accuracy on test: 0.6150\n",
            "\n",
            "Training model - Epoch: 25, LR: 1, Batch_Size: 32\n",
            "Epoch [20/25], Loss: 0.3667\n",
            "Accuracy on test: 0.6500\n",
            "\n",
            "Training model - Epoch: 25, LR: 1, Batch_Size: 64\n",
            "Epoch [20/25], Loss: 0.3991\n",
            "Accuracy on test: 0.7600\n",
            "\n",
            "Training model - Epoch: 25, LR: 1, Batch_Size: 128\n",
            "Epoch [20/25], Loss: 0.2057\n",
            "Accuracy on test: 0.8700\n",
            "\n",
            "Training model - Epoch: 25, LR: 1, Batch_Size: 256\n",
            "Epoch [20/25], Loss: 0.2737\n",
            "Accuracy on test: 0.7750\n",
            "\n",
            "Training model - Epoch: 25, LR: 1, Batch_Size: 512\n",
            "Epoch [20/25], Loss: 0.2118\n",
            "Accuracy on test: 0.8600\n",
            "\n",
            "Training model - Epoch: 25, LR: 0.1, Batch_Size: 16\n",
            "Epoch [20/25], Loss: 0.2034\n",
            "Accuracy on test: 0.9200\n",
            "\n",
            "Training model - Epoch: 25, LR: 0.1, Batch_Size: 32\n",
            "Epoch [20/25], Loss: 0.0928\n",
            "Accuracy on test: 0.9100\n",
            "\n",
            "Training model - Epoch: 25, LR: 0.1, Batch_Size: 64\n",
            "Epoch [20/25], Loss: 0.0813\n",
            "Accuracy on test: 0.9100\n",
            "\n",
            "Training model - Epoch: 25, LR: 0.1, Batch_Size: 128\n",
            "Epoch [20/25], Loss: 0.0980\n",
            "Accuracy on test: 0.8850\n",
            "\n",
            "Training model - Epoch: 25, LR: 0.1, Batch_Size: 256\n",
            "Epoch [20/25], Loss: 0.1201\n",
            "Accuracy on test: 0.8800\n",
            "\n",
            "Training model - Epoch: 25, LR: 0.1, Batch_Size: 512\n",
            "Epoch [20/25], Loss: 0.0824\n",
            "Accuracy on test: 0.8700\n",
            "\n",
            "Training model - Epoch: 25, LR: 0.01, Batch_Size: 16\n",
            "Epoch [20/25], Loss: 0.1103\n",
            "Accuracy on test: 0.9000\n",
            "\n",
            "Training model - Epoch: 25, LR: 0.01, Batch_Size: 32\n",
            "Epoch [20/25], Loss: 0.1091\n",
            "Accuracy on test: 0.9000\n",
            "\n",
            "Training model - Epoch: 25, LR: 0.01, Batch_Size: 64\n",
            "Epoch [20/25], Loss: 0.1142\n",
            "Accuracy on test: 0.9000\n",
            "\n",
            "Training model - Epoch: 25, LR: 0.01, Batch_Size: 128\n",
            "Epoch [20/25], Loss: 0.1621\n",
            "Accuracy on test: 0.9000\n",
            "\n",
            "Training model - Epoch: 25, LR: 0.01, Batch_Size: 256\n",
            "Epoch [20/25], Loss: 0.1961\n",
            "Accuracy on test: 0.8850\n",
            "\n",
            "Training model - Epoch: 25, LR: 0.01, Batch_Size: 512\n",
            "Epoch [20/25], Loss: 0.2961\n",
            "Accuracy on test: 0.8250\n",
            "\n",
            "Training model - Epoch: 25, LR: 0.001, Batch_Size: 16\n",
            "Epoch [20/25], Loss: 0.2181\n",
            "Accuracy on test: 0.8900\n",
            "\n",
            "Training model - Epoch: 25, LR: 0.001, Batch_Size: 32\n",
            "Epoch [20/25], Loss: 0.3337\n",
            "Accuracy on test: 0.7950\n",
            "\n",
            "Training model - Epoch: 25, LR: 0.001, Batch_Size: 64\n",
            "Epoch [20/25], Loss: 0.3527\n",
            "Accuracy on test: 0.7950\n",
            "\n",
            "Training model - Epoch: 25, LR: 0.001, Batch_Size: 128\n",
            "Epoch [20/25], Loss: 0.4530\n",
            "Accuracy on test: 0.7700\n",
            "\n",
            "Training model - Epoch: 25, LR: 0.001, Batch_Size: 256\n",
            "Epoch [20/25], Loss: 0.5915\n",
            "Accuracy on test: 0.7900\n",
            "\n",
            "Training model - Epoch: 25, LR: 0.001, Batch_Size: 512\n",
            "Epoch [20/25], Loss: 0.5860\n",
            "Accuracy on test: 0.6900\n",
            "\n",
            "Training model - Epoch: 25, LR: 0.0001, Batch_Size: 16\n",
            "Epoch [20/25], Loss: 0.5901\n",
            "Accuracy on test: 0.7500\n",
            "\n",
            "Training model - Epoch: 25, LR: 0.0001, Batch_Size: 32\n",
            "Epoch [20/25], Loss: 0.6284\n",
            "Accuracy on test: 0.6650\n",
            "\n",
            "Training model - Epoch: 25, LR: 0.0001, Batch_Size: 64\n",
            "Epoch [20/25], Loss: 0.7219\n",
            "Accuracy on test: 0.4950\n",
            "\n",
            "Training model - Epoch: 25, LR: 0.0001, Batch_Size: 128\n",
            "Epoch [20/25], Loss: 0.6853\n",
            "Accuracy on test: 0.6300\n",
            "\n",
            "Training model - Epoch: 25, LR: 0.0001, Batch_Size: 256\n",
            "Epoch [20/25], Loss: 0.7370\n",
            "Accuracy on test: 0.4900\n",
            "\n",
            "Training model - Epoch: 25, LR: 0.0001, Batch_Size: 512\n",
            "Epoch [20/25], Loss: 0.7101\n",
            "Accuracy on test: 0.4750\n",
            "\n",
            "Training model - Epoch: 50, LR: 10, Batch_Size: 16\n",
            "Epoch [20/50], Loss: 0.6333\n",
            "Epoch [40/50], Loss: 0.4944\n",
            "Accuracy on test: 0.7000\n",
            "\n",
            "Training model - Epoch: 50, LR: 10, Batch_Size: 32\n",
            "Epoch [20/50], Loss: 2.2645\n",
            "Epoch [40/50], Loss: 1.1354\n",
            "Accuracy on test: 0.6150\n",
            "\n",
            "Training model - Epoch: 50, LR: 10, Batch_Size: 64\n",
            "Epoch [20/50], Loss: 0.6662\n",
            "Epoch [40/50], Loss: 0.4879\n",
            "Accuracy on test: 0.6900\n",
            "\n",
            "Training model - Epoch: 50, LR: 10, Batch_Size: 128\n",
            "Epoch [20/50], Loss: 8.0156\n",
            "Epoch [40/50], Loss: 0.4104\n",
            "Accuracy on test: 0.6700\n",
            "\n",
            "Training model - Epoch: 50, LR: 10, Batch_Size: 256\n",
            "Epoch [20/50], Loss: 1.6253\n",
            "Epoch [40/50], Loss: 0.3896\n",
            "Accuracy on test: 0.6800\n",
            "\n",
            "Training model - Epoch: 50, LR: 10, Batch_Size: 512\n",
            "Epoch [20/50], Loss: 20.6084\n",
            "Epoch [40/50], Loss: 0.8754\n",
            "Accuracy on test: 0.7950\n",
            "\n",
            "Training model - Epoch: 50, LR: 1, Batch_Size: 16\n",
            "Epoch [20/50], Loss: 0.7152\n",
            "Epoch [40/50], Loss: 0.7274\n",
            "Accuracy on test: 0.5250\n",
            "\n",
            "Training model - Epoch: 50, LR: 1, Batch_Size: 32\n",
            "Epoch [20/50], Loss: 0.7886\n",
            "Epoch [40/50], Loss: 0.4168\n",
            "Accuracy on test: 0.6700\n",
            "\n",
            "Training model - Epoch: 50, LR: 1, Batch_Size: 64\n",
            "Epoch [20/50], Loss: 0.2629\n",
            "Epoch [40/50], Loss: 0.6940\n",
            "Accuracy on test: 0.6500\n",
            "\n",
            "Training model - Epoch: 50, LR: 1, Batch_Size: 128\n",
            "Epoch [20/50], Loss: 0.2480\n",
            "Epoch [40/50], Loss: 0.2613\n",
            "Accuracy on test: 0.8450\n",
            "\n",
            "Training model - Epoch: 50, LR: 1, Batch_Size: 256\n",
            "Epoch [20/50], Loss: 0.4160\n",
            "Epoch [40/50], Loss: 0.1407\n",
            "Accuracy on test: 0.8300\n",
            "\n",
            "Training model - Epoch: 50, LR: 1, Batch_Size: 512\n",
            "Epoch [20/50], Loss: 0.2255\n",
            "Epoch [40/50], Loss: 0.0950\n",
            "Accuracy on test: 0.8950\n",
            "\n",
            "Training model - Epoch: 50, LR: 0.1, Batch_Size: 16\n",
            "Epoch [20/50], Loss: 0.1851\n",
            "Epoch [40/50], Loss: 0.1371\n",
            "Accuracy on test: 0.8850\n",
            "\n",
            "Training model - Epoch: 50, LR: 0.1, Batch_Size: 32\n",
            "Epoch [20/50], Loss: 0.1282\n",
            "Epoch [40/50], Loss: 0.0974\n",
            "Accuracy on test: 0.8900\n",
            "\n",
            "Training model - Epoch: 50, LR: 0.1, Batch_Size: 64\n",
            "Epoch [20/50], Loss: 0.0979\n",
            "Epoch [40/50], Loss: 0.0359\n",
            "Accuracy on test: 0.9250\n",
            "\n",
            "Training model - Epoch: 50, LR: 0.1, Batch_Size: 128\n",
            "Epoch [20/50], Loss: 0.1089\n",
            "Epoch [40/50], Loss: 0.0733\n",
            "Accuracy on test: 0.9050\n",
            "\n",
            "Training model - Epoch: 50, LR: 0.1, Batch_Size: 256\n",
            "Epoch [20/50], Loss: 0.0853\n",
            "Epoch [40/50], Loss: 0.0491\n",
            "Accuracy on test: 0.9050\n",
            "\n",
            "Training model - Epoch: 50, LR: 0.1, Batch_Size: 512\n",
            "Epoch [20/50], Loss: 0.0837\n",
            "Epoch [40/50], Loss: 0.0335\n",
            "Accuracy on test: 0.8850\n",
            "\n",
            "Training model - Epoch: 50, LR: 0.01, Batch_Size: 16\n",
            "Epoch [20/50], Loss: 0.0856\n",
            "Epoch [40/50], Loss: 0.0549\n",
            "Accuracy on test: 0.9150\n",
            "\n",
            "Training model - Epoch: 50, LR: 0.01, Batch_Size: 32\n",
            "Epoch [20/50], Loss: 0.0988\n",
            "Epoch [40/50], Loss: 0.0585\n",
            "Accuracy on test: 0.9000\n",
            "\n",
            "Training model - Epoch: 50, LR: 0.01, Batch_Size: 64\n",
            "Epoch [20/50], Loss: 0.1238\n",
            "Epoch [40/50], Loss: 0.0784\n",
            "Accuracy on test: 0.8900\n",
            "\n",
            "Training model - Epoch: 50, LR: 0.01, Batch_Size: 128\n",
            "Epoch [20/50], Loss: 0.1654\n",
            "Epoch [40/50], Loss: 0.1071\n",
            "Accuracy on test: 0.9050\n",
            "\n",
            "Training model - Epoch: 50, LR: 0.01, Batch_Size: 256\n",
            "Epoch [20/50], Loss: 0.2187\n",
            "Epoch [40/50], Loss: 0.1205\n",
            "Accuracy on test: 0.9050\n",
            "\n",
            "Training model - Epoch: 50, LR: 0.01, Batch_Size: 512\n",
            "Epoch [20/50], Loss: 0.2818\n",
            "Epoch [40/50], Loss: 0.1774\n",
            "Accuracy on test: 0.9000\n",
            "\n",
            "Training model - Epoch: 50, LR: 0.001, Batch_Size: 16\n",
            "Epoch [20/50], Loss: 0.2285\n",
            "Epoch [40/50], Loss: 0.1631\n",
            "Accuracy on test: 0.9000\n",
            "\n",
            "Training model - Epoch: 50, LR: 0.001, Batch_Size: 32\n",
            "Epoch [20/50], Loss: 0.3017\n",
            "Epoch [40/50], Loss: 0.2214\n",
            "Accuracy on test: 0.8800\n",
            "\n",
            "Training model - Epoch: 50, LR: 0.001, Batch_Size: 64\n",
            "Epoch [20/50], Loss: 0.3669\n",
            "Epoch [40/50], Loss: 0.2406\n",
            "Accuracy on test: 0.8700\n",
            "\n",
            "Training model - Epoch: 50, LR: 0.001, Batch_Size: 128\n",
            "Epoch [20/50], Loss: 0.5190\n",
            "Epoch [40/50], Loss: 0.3440\n",
            "Accuracy on test: 0.8000\n",
            "\n",
            "Training model - Epoch: 50, LR: 0.001, Batch_Size: 256\n",
            "Epoch [20/50], Loss: 0.6172\n",
            "Epoch [40/50], Loss: 0.4868\n",
            "Accuracy on test: 0.7750\n",
            "\n",
            "Training model - Epoch: 50, LR: 0.001, Batch_Size: 512\n",
            "Epoch [20/50], Loss: 0.6996\n",
            "Epoch [40/50], Loss: 0.6478\n",
            "Accuracy on test: 0.7150\n",
            "\n",
            "Training model - Epoch: 50, LR: 0.0001, Batch_Size: 16\n",
            "Epoch [20/50], Loss: 0.6089\n",
            "Epoch [40/50], Loss: 0.4958\n",
            "Accuracy on test: 0.7950\n",
            "\n",
            "Training model - Epoch: 50, LR: 0.0001, Batch_Size: 32\n",
            "Epoch [20/50], Loss: 0.6459\n",
            "Epoch [40/50], Loss: 0.5754\n",
            "Accuracy on test: 0.7800\n",
            "\n",
            "Training model - Epoch: 50, LR: 0.0001, Batch_Size: 64\n",
            "Epoch [20/50], Loss: 0.6800\n",
            "Epoch [40/50], Loss: 0.6353\n",
            "Accuracy on test: 0.6850\n",
            "\n",
            "Training model - Epoch: 50, LR: 0.0001, Batch_Size: 128\n",
            "Epoch [20/50], Loss: 0.7196\n",
            "Epoch [40/50], Loss: 0.7060\n",
            "Accuracy on test: 0.5400\n",
            "\n",
            "Training model - Epoch: 50, LR: 0.0001, Batch_Size: 256\n",
            "Epoch [20/50], Loss: 0.6616\n",
            "Epoch [40/50], Loss: 0.6527\n",
            "Accuracy on test: 0.6150\n",
            "\n",
            "Training model - Epoch: 50, LR: 0.0001, Batch_Size: 512\n",
            "Epoch [20/50], Loss: 0.6883\n",
            "Epoch [40/50], Loss: 0.6844\n",
            "Accuracy on test: 0.5000\n",
            "\n",
            "Training model - Epoch: 100, LR: 10, Batch_Size: 16\n",
            "Epoch [20/100], Loss: 0.7366\n",
            "Epoch [40/100], Loss: 0.6417\n",
            "Epoch [60/100], Loss: 1.0358\n",
            "Epoch [80/100], Loss: 0.8628\n",
            "Epoch [100/100], Loss: 0.7308\n",
            "Accuracy on test: 0.6050\n",
            "\n",
            "Training model - Epoch: 100, LR: 10, Batch_Size: 32\n",
            "Epoch [20/100], Loss: 0.8557\n",
            "Epoch [40/100], Loss: 0.5829\n",
            "Epoch [60/100], Loss: 0.5971\n",
            "Epoch [80/100], Loss: 0.5433\n",
            "Epoch [100/100], Loss: 1.0684\n",
            "Accuracy on test: 0.6650\n",
            "\n",
            "Training model - Epoch: 100, LR: 10, Batch_Size: 64\n",
            "Epoch [20/100], Loss: 11.1210\n",
            "Epoch [40/100], Loss: 0.6019\n",
            "Epoch [60/100], Loss: 0.4545\n",
            "Epoch [80/100], Loss: 0.8130\n",
            "Epoch [100/100], Loss: 0.4586\n",
            "Accuracy on test: 0.6750\n",
            "\n",
            "Training model - Epoch: 100, LR: 10, Batch_Size: 128\n",
            "Epoch [20/100], Loss: 0.2941\n",
            "Epoch [40/100], Loss: 0.2769\n",
            "Epoch [60/100], Loss: 0.3154\n",
            "Epoch [80/100], Loss: 0.3582\n",
            "Epoch [100/100], Loss: 0.4763\n",
            "Accuracy on test: 0.7350\n",
            "\n",
            "Training model - Epoch: 100, LR: 10, Batch_Size: 256\n",
            "Epoch [20/100], Loss: 2.6916\n",
            "Epoch [40/100], Loss: 0.2898\n",
            "Epoch [60/100], Loss: 0.3412\n",
            "Epoch [80/100], Loss: 0.3429\n",
            "Epoch [100/100], Loss: 0.3544\n",
            "Accuracy on test: 0.7350\n",
            "\n",
            "Training model - Epoch: 100, LR: 10, Batch_Size: 512\n",
            "Epoch [20/100], Loss: 7.2612\n",
            "Epoch [40/100], Loss: 0.2679\n",
            "Epoch [60/100], Loss: 0.2482\n",
            "Epoch [80/100], Loss: 0.2156\n",
            "Epoch [100/100], Loss: 0.2264\n",
            "Accuracy on test: 0.8100\n",
            "\n",
            "Training model - Epoch: 100, LR: 1, Batch_Size: 16\n",
            "Epoch [20/100], Loss: 0.4322\n",
            "Epoch [40/100], Loss: 0.4549\n",
            "Epoch [60/100], Loss: 0.4991\n",
            "Epoch [80/100], Loss: 0.5001\n",
            "Epoch [100/100], Loss: 0.5132\n",
            "Accuracy on test: 0.5900\n",
            "\n",
            "Training model - Epoch: 100, LR: 1, Batch_Size: 32\n",
            "Epoch [20/100], Loss: 0.8098\n",
            "Epoch [40/100], Loss: 0.3690\n",
            "Epoch [60/100], Loss: 0.5461\n",
            "Epoch [80/100], Loss: 0.5833\n",
            "Epoch [100/100], Loss: 0.6330\n",
            "Accuracy on test: 0.6150\n",
            "\n",
            "Training model - Epoch: 100, LR: 1, Batch_Size: 64\n",
            "Epoch [20/100], Loss: 0.2400\n",
            "Epoch [40/100], Loss: 0.3887\n",
            "Epoch [60/100], Loss: 0.3754\n",
            "Epoch [80/100], Loss: 0.4085\n",
            "Epoch [100/100], Loss: 0.3919\n",
            "Accuracy on test: 0.6700\n",
            "\n",
            "Training model - Epoch: 100, LR: 1, Batch_Size: 128\n",
            "Epoch [20/100], Loss: 0.2794\n",
            "Epoch [40/100], Loss: 0.2664\n",
            "Epoch [60/100], Loss: 0.2881\n",
            "Epoch [80/100], Loss: 0.4186\n",
            "Epoch [100/100], Loss: 0.3250\n",
            "Accuracy on test: 0.7350\n",
            "\n",
            "Training model - Epoch: 100, LR: 1, Batch_Size: 256\n",
            "Epoch [20/100], Loss: 0.2373\n",
            "Epoch [40/100], Loss: 0.1822\n",
            "Epoch [60/100], Loss: 0.3651\n",
            "Epoch [80/100], Loss: 0.3519\n",
            "Epoch [100/100], Loss: 0.3321\n",
            "Accuracy on test: 0.7750\n",
            "\n",
            "Training model - Epoch: 100, LR: 1, Batch_Size: 512\n",
            "Epoch [20/100], Loss: 0.3004\n",
            "Epoch [40/100], Loss: 0.1968\n",
            "Epoch [60/100], Loss: 0.1220\n",
            "Epoch [80/100], Loss: 0.1481\n",
            "Epoch [100/100], Loss: 0.1737\n",
            "Accuracy on test: 0.8400\n",
            "\n",
            "Training model - Epoch: 100, LR: 0.1, Batch_Size: 16\n",
            "Epoch [20/100], Loss: 0.1644\n",
            "Epoch [40/100], Loss: 0.1352\n",
            "Epoch [60/100], Loss: 0.1339\n",
            "Epoch [80/100], Loss: 0.1195\n",
            "Epoch [100/100], Loss: 0.1268\n",
            "Accuracy on test: 0.9000\n",
            "\n",
            "Training model - Epoch: 100, LR: 0.1, Batch_Size: 32\n",
            "Epoch [20/100], Loss: 0.1117\n",
            "Epoch [40/100], Loss: 0.1528\n",
            "Epoch [60/100], Loss: 0.0712\n",
            "Epoch [80/100], Loss: 0.1368\n",
            "Epoch [100/100], Loss: 0.0934\n",
            "Accuracy on test: 0.9200\n",
            "\n",
            "Training model - Epoch: 100, LR: 0.1, Batch_Size: 64\n",
            "Epoch [20/100], Loss: 0.1482\n",
            "Epoch [40/100], Loss: 0.1236\n",
            "Epoch [60/100], Loss: 0.0517\n",
            "Epoch [80/100], Loss: 0.0260\n",
            "Epoch [100/100], Loss: 0.0400\n",
            "Accuracy on test: 0.8800\n",
            "\n",
            "Training model - Epoch: 100, LR: 0.1, Batch_Size: 128\n",
            "Epoch [20/100], Loss: 0.0758\n",
            "Epoch [40/100], Loss: 0.0352\n",
            "Epoch [60/100], Loss: 0.2323\n",
            "Epoch [80/100], Loss: 0.0168\n",
            "Epoch [100/100], Loss: 0.0036\n",
            "Accuracy on test: 0.8700\n",
            "\n",
            "Training model - Epoch: 100, LR: 0.1, Batch_Size: 256\n",
            "Epoch [20/100], Loss: 0.1609\n",
            "Epoch [40/100], Loss: 0.0657\n",
            "Epoch [60/100], Loss: 0.0316\n",
            "Epoch [80/100], Loss: 0.0297\n",
            "Epoch [100/100], Loss: 0.0175\n",
            "Accuracy on test: 0.9050\n",
            "\n",
            "Training model - Epoch: 100, LR: 0.1, Batch_Size: 512\n",
            "Epoch [20/100], Loss: 0.0900\n",
            "Epoch [40/100], Loss: 0.0419\n",
            "Epoch [60/100], Loss: 0.0395\n",
            "Epoch [80/100], Loss: 0.0143\n",
            "Epoch [100/100], Loss: 0.0058\n",
            "Accuracy on test: 0.8700\n",
            "\n",
            "Training model - Epoch: 100, LR: 0.01, Batch_Size: 16\n",
            "Epoch [20/100], Loss: 0.0997\n",
            "Epoch [40/100], Loss: 0.0589\n",
            "Epoch [60/100], Loss: 0.0413\n",
            "Epoch [80/100], Loss: 0.0366\n",
            "Epoch [100/100], Loss: 0.0451\n",
            "Accuracy on test: 0.9050\n",
            "\n",
            "Training model - Epoch: 100, LR: 0.01, Batch_Size: 32\n",
            "Epoch [20/100], Loss: 0.1164\n",
            "Epoch [40/100], Loss: 0.0686\n",
            "Epoch [60/100], Loss: 0.0616\n",
            "Epoch [80/100], Loss: 0.0464\n",
            "Epoch [100/100], Loss: 0.0443\n",
            "Accuracy on test: 0.8700\n",
            "\n",
            "Training model - Epoch: 100, LR: 0.01, Batch_Size: 64\n",
            "Epoch [20/100], Loss: 0.1242\n",
            "Epoch [40/100], Loss: 0.0771\n",
            "Epoch [60/100], Loss: 0.0544\n",
            "Epoch [80/100], Loss: 0.0411\n",
            "Epoch [100/100], Loss: 0.0297\n",
            "Accuracy on test: 0.9100\n",
            "\n",
            "Training model - Epoch: 100, LR: 0.01, Batch_Size: 128\n",
            "Epoch [20/100], Loss: 0.2016\n",
            "Epoch [40/100], Loss: 0.1098\n",
            "Epoch [60/100], Loss: 0.0757\n",
            "Epoch [80/100], Loss: 0.0794\n",
            "Epoch [100/100], Loss: 0.0547\n",
            "Accuracy on test: 0.8900\n",
            "\n",
            "Training model - Epoch: 100, LR: 0.01, Batch_Size: 256\n",
            "Epoch [20/100], Loss: 0.2577\n",
            "Epoch [40/100], Loss: 0.1387\n",
            "Epoch [60/100], Loss: 0.1196\n",
            "Epoch [80/100], Loss: 0.1340\n",
            "Epoch [100/100], Loss: 0.0736\n",
            "Accuracy on test: 0.9100\n",
            "\n",
            "Training model - Epoch: 100, LR: 0.01, Batch_Size: 512\n",
            "Epoch [20/100], Loss: 0.2951\n",
            "Epoch [40/100], Loss: 0.1754\n",
            "Epoch [60/100], Loss: 0.1293\n",
            "Epoch [80/100], Loss: 0.0950\n",
            "Epoch [100/100], Loss: 0.0772\n",
            "Accuracy on test: 0.9150\n",
            "\n",
            "Training model - Epoch: 100, LR: 0.001, Batch_Size: 16\n",
            "Epoch [20/100], Loss: 0.2366\n",
            "Epoch [40/100], Loss: 0.1713\n",
            "Epoch [60/100], Loss: 0.1345\n",
            "Epoch [80/100], Loss: 0.1124\n",
            "Epoch [100/100], Loss: 0.0978\n",
            "Accuracy on test: 0.9100\n",
            "\n",
            "Training model - Epoch: 100, LR: 0.001, Batch_Size: 32\n",
            "Epoch [20/100], Loss: 0.2905\n",
            "Epoch [40/100], Loss: 0.2084\n",
            "Epoch [60/100], Loss: 0.1706\n",
            "Epoch [80/100], Loss: 0.1466\n",
            "Epoch [100/100], Loss: 0.1304\n",
            "Accuracy on test: 0.8950\n",
            "\n",
            "Training model - Epoch: 100, LR: 0.001, Batch_Size: 64\n",
            "Epoch [20/100], Loss: 0.3864\n",
            "Epoch [40/100], Loss: 0.2748\n",
            "Epoch [60/100], Loss: 0.2239\n",
            "Epoch [80/100], Loss: 0.1941\n",
            "Epoch [100/100], Loss: 0.1747\n",
            "Accuracy on test: 0.8900\n",
            "\n",
            "Training model - Epoch: 100, LR: 0.001, Batch_Size: 128\n",
            "Epoch [20/100], Loss: 0.4402\n",
            "Epoch [40/100], Loss: 0.3353\n",
            "Epoch [60/100], Loss: 0.2677\n",
            "Epoch [80/100], Loss: 0.2383\n",
            "Epoch [100/100], Loss: 0.2174\n",
            "Accuracy on test: 0.8650\n",
            "\n",
            "Training model - Epoch: 100, LR: 0.001, Batch_Size: 256\n",
            "Epoch [20/100], Loss: 0.5742\n",
            "Epoch [40/100], Loss: 0.4576\n",
            "Epoch [60/100], Loss: 0.3876\n",
            "Epoch [80/100], Loss: 0.3033\n",
            "Epoch [100/100], Loss: 0.2994\n",
            "Accuracy on test: 0.8200\n",
            "\n",
            "Training model - Epoch: 100, LR: 0.001, Batch_Size: 512\n",
            "Epoch [20/100], Loss: 0.6815\n",
            "Epoch [40/100], Loss: 0.6140\n",
            "Epoch [60/100], Loss: 0.5432\n",
            "Epoch [80/100], Loss: 0.4614\n",
            "Epoch [100/100], Loss: 0.4033\n",
            "Accuracy on test: 0.7900\n",
            "\n",
            "Training model - Epoch: 100, LR: 0.0001, Batch_Size: 16\n",
            "Epoch [20/100], Loss: 0.6220\n",
            "Epoch [40/100], Loss: 0.4987\n",
            "Epoch [60/100], Loss: 0.4032\n",
            "Epoch [80/100], Loss: 0.3518\n",
            "Epoch [100/100], Loss: 0.3207\n",
            "Accuracy on test: 0.7950\n",
            "\n",
            "Training model - Epoch: 100, LR: 0.0001, Batch_Size: 32\n",
            "Epoch [20/100], Loss: 0.6414\n",
            "Epoch [40/100], Loss: 0.5605\n",
            "Epoch [60/100], Loss: 0.4832\n",
            "Epoch [80/100], Loss: 0.4229\n",
            "Epoch [100/100], Loss: 0.3812\n",
            "Accuracy on test: 0.8050\n",
            "\n",
            "Training model - Epoch: 100, LR: 0.0001, Batch_Size: 64\n",
            "Epoch [20/100], Loss: 0.6950\n",
            "Epoch [40/100], Loss: 0.6573\n",
            "Epoch [60/100], Loss: 0.6143\n",
            "Epoch [80/100], Loss: 0.5687\n",
            "Epoch [100/100], Loss: 0.5235\n",
            "Accuracy on test: 0.7650\n",
            "\n",
            "Training model - Epoch: 100, LR: 0.0001, Batch_Size: 128\n",
            "Epoch [20/100], Loss: 0.6778\n",
            "Epoch [40/100], Loss: 0.6583\n",
            "Epoch [60/100], Loss: 0.6345\n",
            "Epoch [80/100], Loss: 0.6100\n",
            "Epoch [100/100], Loss: 0.5795\n",
            "Accuracy on test: 0.6950\n",
            "\n",
            "Training model - Epoch: 100, LR: 0.0001, Batch_Size: 256\n",
            "Epoch [20/100], Loss: 0.6851\n",
            "Epoch [40/100], Loss: 0.6618\n",
            "Epoch [60/100], Loss: 0.6596\n",
            "Epoch [80/100], Loss: 0.6477\n",
            "Epoch [100/100], Loss: 0.6309\n",
            "Accuracy on test: 0.6050\n",
            "\n",
            "Training model - Epoch: 100, LR: 0.0001, Batch_Size: 512\n",
            "Epoch [20/100], Loss: 0.7229\n",
            "Epoch [40/100], Loss: 0.7136\n",
            "Epoch [60/100], Loss: 0.7077\n",
            "Epoch [80/100], Loss: 0.7006\n",
            "Epoch [100/100], Loss: 0.6886\n",
            "Accuracy on test: 0.5200\n",
            "\n",
            "Training model - Epoch: 250, LR: 10, Batch_Size: 16\n",
            "Epoch [20/250], Loss: 0.8637\n",
            "Epoch [40/250], Loss: 0.7553\n",
            "Epoch [60/250], Loss: 0.7048\n",
            "Epoch [80/250], Loss: 0.7428\n",
            "Epoch [100/250], Loss: 1.0522\n",
            "Epoch [120/250], Loss: 0.9562\n",
            "Epoch [140/250], Loss: 1.1293\n",
            "Epoch [160/250], Loss: 0.8537\n",
            "Epoch [180/250], Loss: 0.8179\n",
            "Epoch [200/250], Loss: 0.9473\n",
            "Epoch [220/250], Loss: 0.6724\n",
            "Epoch [240/250], Loss: 0.8967\n",
            "Accuracy on test: 0.6100\n",
            "\n",
            "Training model - Epoch: 250, LR: 10, Batch_Size: 32\n",
            "Epoch [20/250], Loss: 0.6702\n",
            "Epoch [40/250], Loss: 0.4694\n",
            "Epoch [60/250], Loss: 0.7657\n",
            "Epoch [80/250], Loss: 0.5653\n",
            "Epoch [100/250], Loss: 0.4590\n",
            "Epoch [120/250], Loss: 0.6497\n",
            "Epoch [140/250], Loss: 0.6135\n",
            "Epoch [160/250], Loss: 0.9006\n",
            "Epoch [180/250], Loss: 0.5874\n",
            "Epoch [200/250], Loss: 0.7781\n",
            "Epoch [220/250], Loss: 0.5318\n",
            "Epoch [240/250], Loss: 0.8274\n",
            "Accuracy on test: 0.6600\n",
            "\n",
            "Training model - Epoch: 250, LR: 10, Batch_Size: 64\n",
            "Epoch [20/250], Loss: 14.7297\n",
            "Epoch [40/250], Loss: 0.6476\n",
            "Epoch [60/250], Loss: 0.7912\n",
            "Epoch [80/250], Loss: 0.4587\n",
            "Epoch [100/250], Loss: 0.5317\n",
            "Epoch [120/250], Loss: 0.4271\n",
            "Epoch [140/250], Loss: 1.0279\n",
            "Epoch [160/250], Loss: 0.5021\n",
            "Epoch [180/250], Loss: 0.4647\n",
            "Epoch [200/250], Loss: 0.5993\n",
            "Epoch [220/250], Loss: 0.5316\n",
            "Epoch [240/250], Loss: 0.5308\n",
            "Accuracy on test: 0.6350\n",
            "\n",
            "Training model - Epoch: 250, LR: 10, Batch_Size: 128\n",
            "Epoch [20/250], Loss: 1.2517\n",
            "Epoch [40/250], Loss: 0.3926\n",
            "Epoch [60/250], Loss: 0.3560\n",
            "Epoch [80/250], Loss: 0.5043\n",
            "Epoch [100/250], Loss: 0.3475\n",
            "Epoch [120/250], Loss: 0.2975\n",
            "Epoch [140/250], Loss: 0.3238\n",
            "Epoch [160/250], Loss: 0.3313\n",
            "Epoch [180/250], Loss: 0.4236\n",
            "Epoch [200/250], Loss: 0.8048\n",
            "Epoch [220/250], Loss: 0.3327\n",
            "Epoch [240/250], Loss: 0.3830\n",
            "Accuracy on test: 0.7300\n",
            "\n",
            "Training model - Epoch: 250, LR: 10, Batch_Size: 256\n",
            "Epoch [20/250], Loss: 3.9435\n",
            "Epoch [40/250], Loss: 0.3778\n",
            "Epoch [60/250], Loss: 0.3179\n",
            "Epoch [80/250], Loss: 0.5530\n",
            "Epoch [100/250], Loss: 0.3580\n",
            "Epoch [120/250], Loss: 0.3026\n",
            "Epoch [140/250], Loss: 0.8795\n",
            "Epoch [160/250], Loss: 0.2895\n",
            "Epoch [180/250], Loss: 0.2799\n",
            "Epoch [200/250], Loss: 0.2912\n",
            "Epoch [220/250], Loss: 0.2750\n",
            "Epoch [240/250], Loss: 0.3080\n",
            "Accuracy on test: 0.7200\n",
            "\n",
            "Training model - Epoch: 250, LR: 10, Batch_Size: 512\n",
            "Epoch [20/250], Loss: 16.2619\n",
            "Epoch [40/250], Loss: 9.2533\n",
            "Epoch [60/250], Loss: 0.3039\n",
            "Epoch [80/250], Loss: 0.2879\n",
            "Epoch [100/250], Loss: 0.2762\n",
            "Epoch [120/250], Loss: 0.3087\n",
            "Epoch [140/250], Loss: 0.2941\n",
            "Epoch [160/250], Loss: 0.2980\n",
            "Epoch [180/250], Loss: 0.3023\n",
            "Epoch [200/250], Loss: 0.2994\n",
            "Epoch [220/250], Loss: 0.2945\n",
            "Epoch [240/250], Loss: 0.7160\n",
            "Accuracy on test: 0.7200\n",
            "\n",
            "Training model - Epoch: 250, LR: 1, Batch_Size: 16\n",
            "Epoch [20/250], Loss: 0.5181\n",
            "Epoch [40/250], Loss: 0.5070\n",
            "Epoch [60/250], Loss: 0.5040\n",
            "Epoch [80/250], Loss: 0.5145\n",
            "Epoch [100/250], Loss: 0.4758\n",
            "Epoch [120/250], Loss: 0.4854\n",
            "Epoch [140/250], Loss: 0.5402\n",
            "Epoch [160/250], Loss: 0.5545\n",
            "Epoch [180/250], Loss: 0.4807\n",
            "Epoch [200/250], Loss: 0.4834\n",
            "Epoch [220/250], Loss: 0.4890\n",
            "Epoch [240/250], Loss: 0.4833\n",
            "Accuracy on test: 0.6050\n",
            "\n",
            "Training model - Epoch: 250, LR: 1, Batch_Size: 32\n",
            "Epoch [20/250], Loss: 0.3225\n",
            "Epoch [40/250], Loss: 0.6701\n",
            "Epoch [60/250], Loss: 0.5243\n",
            "Epoch [80/250], Loss: 0.5100\n",
            "Epoch [100/250], Loss: 0.5243\n",
            "Epoch [120/250], Loss: 0.5405\n",
            "Epoch [140/250], Loss: 0.4990\n",
            "Epoch [160/250], Loss: 0.5377\n",
            "Epoch [180/250], Loss: 0.4985\n",
            "Epoch [200/250], Loss: 0.5018\n",
            "Epoch [220/250], Loss: 0.5111\n",
            "Epoch [240/250], Loss: 0.5367\n",
            "Accuracy on test: 0.5700\n",
            "\n",
            "Training model - Epoch: 250, LR: 1, Batch_Size: 64\n",
            "Epoch [20/250], Loss: 0.4592\n",
            "Epoch [40/250], Loss: 0.3556\n",
            "Epoch [60/250], Loss: 0.5746\n",
            "Epoch [80/250], Loss: 0.5594\n",
            "Epoch [100/250], Loss: 0.4889\n",
            "Epoch [120/250], Loss: 0.4665\n",
            "Epoch [140/250], Loss: 0.4727\n",
            "Epoch [160/250], Loss: 0.4777\n",
            "Epoch [180/250], Loss: 0.4909\n",
            "Epoch [200/250], Loss: 0.4652\n",
            "Epoch [220/250], Loss: 0.4595\n",
            "Epoch [240/250], Loss: 0.4636\n",
            "Accuracy on test: 0.6150\n",
            "\n",
            "Training model - Epoch: 250, LR: 1, Batch_Size: 128\n",
            "Epoch [20/250], Loss: 0.2649\n",
            "Epoch [40/250], Loss: 0.4118\n",
            "Epoch [60/250], Loss: 0.3264\n",
            "Epoch [80/250], Loss: 0.3112\n",
            "Epoch [100/250], Loss: 0.2546\n",
            "Epoch [120/250], Loss: 0.4505\n",
            "Epoch [140/250], Loss: 0.3422\n",
            "Epoch [160/250], Loss: 0.3278\n",
            "Epoch [180/250], Loss: 0.3274\n",
            "Epoch [200/250], Loss: 0.3176\n",
            "Epoch [220/250], Loss: 0.3137\n",
            "Epoch [240/250], Loss: 0.3123\n",
            "Accuracy on test: 0.7150\n",
            "\n",
            "Training model - Epoch: 250, LR: 1, Batch_Size: 256\n",
            "Epoch [20/250], Loss: 0.2447\n",
            "Epoch [40/250], Loss: 0.1380\n",
            "Epoch [60/250], Loss: 0.9215\n",
            "Epoch [80/250], Loss: 0.3178\n",
            "Epoch [100/250], Loss: 0.2710\n",
            "Epoch [120/250], Loss: 0.2331\n",
            "Epoch [140/250], Loss: 0.3261\n",
            "Epoch [160/250], Loss: 0.6150\n",
            "Epoch [180/250], Loss: 0.3278\n",
            "Epoch [200/250], Loss: 0.3251\n",
            "Epoch [220/250], Loss: 0.2758\n",
            "Epoch [240/250], Loss: 0.3153\n",
            "Accuracy on test: 0.7600\n",
            "\n",
            "Training model - Epoch: 250, LR: 1, Batch_Size: 512\n",
            "Epoch [20/250], Loss: 0.2179\n",
            "Epoch [40/250], Loss: 0.1455\n",
            "Epoch [60/250], Loss: 0.1366\n",
            "Epoch [80/250], Loss: 0.2259\n",
            "Epoch [100/250], Loss: 0.3271\n",
            "Epoch [120/250], Loss: 0.1261\n",
            "Epoch [140/250], Loss: 0.1166\n",
            "Epoch [160/250], Loss: 0.1094\n",
            "Epoch [180/250], Loss: 0.0950\n",
            "Epoch [200/250], Loss: 0.1117\n",
            "Epoch [220/250], Loss: 0.1154\n",
            "Epoch [240/250], Loss: 0.0791\n",
            "Accuracy on test: 0.8900\n",
            "\n",
            "Training model - Epoch: 250, LR: 0.1, Batch_Size: 16\n",
            "Epoch [20/250], Loss: 0.1706\n",
            "Epoch [40/250], Loss: 0.1337\n",
            "Epoch [60/250], Loss: 0.1820\n",
            "Epoch [80/250], Loss: 0.1132\n",
            "Epoch [100/250], Loss: 0.1538\n",
            "Epoch [120/250], Loss: 0.1856\n",
            "Epoch [140/250], Loss: 0.1257\n",
            "Epoch [160/250], Loss: 0.1870\n",
            "Epoch [180/250], Loss: 0.1595\n",
            "Epoch [200/250], Loss: 0.1685\n",
            "Epoch [220/250], Loss: 0.1456\n",
            "Epoch [240/250], Loss: 0.1167\n",
            "Accuracy on test: 0.8600\n",
            "\n",
            "Training model - Epoch: 250, LR: 0.1, Batch_Size: 32\n",
            "Epoch [20/250], Loss: 0.1545\n",
            "Epoch [40/250], Loss: 0.0866\n",
            "Epoch [60/250], Loss: 0.0376\n",
            "Epoch [80/250], Loss: 0.0415\n",
            "Epoch [100/250], Loss: 0.0647\n",
            "Epoch [120/250], Loss: 0.2160\n",
            "Epoch [140/250], Loss: 0.0451\n",
            "Epoch [160/250], Loss: 0.0439\n",
            "Epoch [180/250], Loss: 0.1425\n",
            "Epoch [200/250], Loss: 0.0863\n",
            "Epoch [220/250], Loss: 0.0777\n",
            "Epoch [240/250], Loss: 0.0654\n",
            "Accuracy on test: 0.8800\n",
            "\n",
            "Training model - Epoch: 250, LR: 0.1, Batch_Size: 64\n",
            "Epoch [20/250], Loss: 0.1369\n",
            "Epoch [40/250], Loss: 0.1491\n",
            "Epoch [60/250], Loss: 0.1052\n",
            "Epoch [80/250], Loss: 0.0160\n",
            "Epoch [100/250], Loss: 0.0150\n",
            "Epoch [120/250], Loss: 0.0414\n",
            "Epoch [140/250], Loss: 0.0208\n",
            "Epoch [160/250], Loss: 0.0235\n",
            "Epoch [180/250], Loss: 0.0197\n",
            "Epoch [200/250], Loss: 0.0475\n",
            "Epoch [220/250], Loss: 0.0295\n",
            "Epoch [240/250], Loss: 0.0421\n",
            "Accuracy on test: 0.9200\n",
            "\n",
            "Training model - Epoch: 250, LR: 0.1, Batch_Size: 128\n",
            "Epoch [20/250], Loss: 0.0777\n",
            "Epoch [40/250], Loss: 0.0466\n",
            "Epoch [60/250], Loss: 0.0175\n",
            "Epoch [80/250], Loss: 0.0046\n",
            "Epoch [100/250], Loss: 0.0033\n",
            "Epoch [120/250], Loss: 0.0026\n",
            "Epoch [140/250], Loss: 0.0029\n",
            "Epoch [160/250], Loss: 0.0023\n",
            "Epoch [180/250], Loss: 0.0022\n",
            "Epoch [200/250], Loss: 0.0020\n",
            "Epoch [220/250], Loss: 0.0019\n",
            "Epoch [240/250], Loss: 0.0018\n",
            "Accuracy on test: 0.9150\n",
            "\n",
            "Training model - Epoch: 250, LR: 0.1, Batch_Size: 256\n",
            "Epoch [20/250], Loss: 0.1441\n",
            "Epoch [40/250], Loss: 0.0687\n",
            "Epoch [60/250], Loss: 0.0520\n",
            "Epoch [80/250], Loss: 0.0650\n",
            "Epoch [100/250], Loss: 0.0430\n",
            "Epoch [120/250], Loss: 0.0108\n",
            "Epoch [140/250], Loss: 0.0044\n",
            "Epoch [160/250], Loss: 0.0023\n",
            "Epoch [180/250], Loss: 0.0029\n",
            "Epoch [200/250], Loss: 0.0013\n",
            "Epoch [220/250], Loss: 0.0010\n",
            "Epoch [240/250], Loss: 0.0008\n",
            "Accuracy on test: 0.8950\n",
            "\n",
            "Training model - Epoch: 250, LR: 0.1, Batch_Size: 512\n",
            "Epoch [20/250], Loss: 0.0929\n",
            "Epoch [40/250], Loss: 0.0355\n",
            "Epoch [60/250], Loss: 0.0164\n",
            "Epoch [80/250], Loss: 0.0076\n",
            "Epoch [100/250], Loss: 0.0043\n",
            "Epoch [120/250], Loss: 0.0027\n",
            "Epoch [140/250], Loss: 0.0022\n",
            "Epoch [160/250], Loss: 0.0015\n",
            "Epoch [180/250], Loss: 0.0012\n",
            "Epoch [200/250], Loss: 0.0010\n",
            "Epoch [220/250], Loss: 0.0008\n",
            "Epoch [240/250], Loss: 0.0006\n",
            "Accuracy on test: 0.8850\n",
            "\n",
            "Training model - Epoch: 250, LR: 0.01, Batch_Size: 16\n",
            "Epoch [20/250], Loss: 0.0822\n",
            "Epoch [40/250], Loss: 0.0509\n",
            "Epoch [60/250], Loss: 0.0504\n",
            "Epoch [80/250], Loss: 0.0209\n",
            "Epoch [100/250], Loss: 0.0139\n",
            "Epoch [120/250], Loss: 0.0078\n",
            "Epoch [140/250], Loss: 0.0057\n",
            "Epoch [160/250], Loss: 0.0041\n",
            "Epoch [180/250], Loss: 0.0030\n",
            "Epoch [200/250], Loss: 0.0018\n",
            "Epoch [220/250], Loss: 0.0013\n",
            "Epoch [240/250], Loss: 0.1770\n",
            "Accuracy on test: 0.8800\n",
            "\n",
            "Training model - Epoch: 250, LR: 0.01, Batch_Size: 32\n",
            "Epoch [20/250], Loss: 0.0922\n",
            "Epoch [40/250], Loss: 0.0581\n",
            "Epoch [60/250], Loss: 0.0348\n",
            "Epoch [80/250], Loss: 0.0212\n",
            "Epoch [100/250], Loss: 0.0149\n",
            "Epoch [120/250], Loss: 0.0264\n",
            "Epoch [140/250], Loss: 0.0064\n",
            "Epoch [160/250], Loss: 0.0050\n",
            "Epoch [180/250], Loss: 0.0123\n",
            "Epoch [200/250], Loss: 0.0026\n",
            "Epoch [220/250], Loss: 0.0019\n",
            "Epoch [240/250], Loss: 0.0015\n",
            "Accuracy on test: 0.8800\n",
            "\n",
            "Training model - Epoch: 250, LR: 0.01, Batch_Size: 64\n",
            "Epoch [20/250], Loss: 0.1146\n",
            "Epoch [40/250], Loss: 0.0708\n",
            "Epoch [60/250], Loss: 0.0476\n",
            "Epoch [80/250], Loss: 0.0344\n",
            "Epoch [100/250], Loss: 0.0281\n",
            "Epoch [120/250], Loss: 0.0207\n",
            "Epoch [140/250], Loss: 0.0171\n",
            "Epoch [160/250], Loss: 0.0212\n",
            "Epoch [180/250], Loss: 0.0137\n",
            "Epoch [200/250], Loss: 0.0087\n",
            "Epoch [220/250], Loss: 0.0172\n",
            "Epoch [240/250], Loss: 0.0060\n",
            "Accuracy on test: 0.8800\n",
            "\n",
            "Training model - Epoch: 250, LR: 0.01, Batch_Size: 128\n",
            "Epoch [20/250], Loss: 0.1629\n",
            "Epoch [40/250], Loss: 0.1007\n",
            "Epoch [60/250], Loss: 0.0821\n",
            "Epoch [80/250], Loss: 0.0716\n",
            "Epoch [100/250], Loss: 0.0476\n",
            "Epoch [120/250], Loss: 0.0443\n",
            "Epoch [140/250], Loss: 0.0343\n",
            "Epoch [160/250], Loss: 0.0297\n",
            "Epoch [180/250], Loss: 0.0210\n",
            "Epoch [200/250], Loss: 0.0220\n",
            "Epoch [220/250], Loss: 0.0178\n",
            "Epoch [240/250], Loss: 0.0132\n",
            "Accuracy on test: 0.9050\n",
            "\n",
            "Training model - Epoch: 250, LR: 0.01, Batch_Size: 256\n",
            "Epoch [20/250], Loss: 0.1992\n",
            "Epoch [40/250], Loss: 0.1338\n",
            "Epoch [60/250], Loss: 0.1075\n",
            "Epoch [80/250], Loss: 0.0865\n",
            "Epoch [100/250], Loss: 0.0737\n",
            "Epoch [120/250], Loss: 0.0648\n",
            "Epoch [140/250], Loss: 0.0640\n",
            "Epoch [160/250], Loss: 0.0743\n",
            "Epoch [180/250], Loss: 0.0510\n",
            "Epoch [200/250], Loss: 0.0546\n",
            "Epoch [220/250], Loss: 0.0403\n",
            "Epoch [240/250], Loss: 0.0603\n",
            "Accuracy on test: 0.9050\n",
            "\n",
            "Training model - Epoch: 250, LR: 0.01, Batch_Size: 512\n",
            "Epoch [20/250], Loss: 0.2874\n",
            "Epoch [40/250], Loss: 0.1759\n",
            "Epoch [60/250], Loss: 0.1271\n",
            "Epoch [80/250], Loss: 0.0957\n",
            "Epoch [100/250], Loss: 0.0837\n",
            "Epoch [120/250], Loss: 0.0720\n",
            "Epoch [140/250], Loss: 0.0655\n",
            "Epoch [160/250], Loss: 0.0553\n",
            "Epoch [180/250], Loss: 0.0519\n",
            "Epoch [200/250], Loss: 0.0420\n",
            "Epoch [220/250], Loss: 0.0383\n",
            "Epoch [240/250], Loss: 0.0361\n",
            "Accuracy on test: 0.9000\n",
            "\n",
            "Training model - Epoch: 250, LR: 0.001, Batch_Size: 16\n",
            "Epoch [20/250], Loss: 0.2390\n",
            "Epoch [40/250], Loss: 0.1717\n",
            "Epoch [60/250], Loss: 0.1347\n",
            "Epoch [80/250], Loss: 0.1134\n",
            "Epoch [100/250], Loss: 0.0983\n",
            "Epoch [120/250], Loss: 0.0874\n",
            "Epoch [140/250], Loss: 0.0774\n",
            "Epoch [160/250], Loss: 0.0709\n",
            "Epoch [180/250], Loss: 0.0641\n",
            "Epoch [200/250], Loss: 0.0582\n",
            "Epoch [220/250], Loss: 0.0529\n",
            "Epoch [240/250], Loss: 0.0471\n",
            "Accuracy on test: 0.8900\n",
            "\n",
            "Training model - Epoch: 250, LR: 0.001, Batch_Size: 32\n",
            "Epoch [20/250], Loss: 0.3033\n",
            "Epoch [40/250], Loss: 0.2150\n",
            "Epoch [60/250], Loss: 0.1641\n",
            "Epoch [80/250], Loss: 0.1347\n",
            "Epoch [100/250], Loss: 0.1158\n",
            "Epoch [120/250], Loss: 0.1049\n",
            "Epoch [140/250], Loss: 0.0958\n",
            "Epoch [160/250], Loss: 0.0893\n",
            "Epoch [180/250], Loss: 0.0841\n",
            "Epoch [200/250], Loss: 0.0798\n",
            "Epoch [220/250], Loss: 0.0751\n",
            "Epoch [240/250], Loss: 0.0722\n",
            "Accuracy on test: 0.8900\n",
            "\n",
            "Training model - Epoch: 250, LR: 0.001, Batch_Size: 64\n",
            "Epoch [20/250], Loss: 0.3592\n",
            "Epoch [40/250], Loss: 0.2665\n",
            "Epoch [60/250], Loss: 0.2127\n",
            "Epoch [80/250], Loss: 0.1809\n",
            "Epoch [100/250], Loss: 0.1574\n",
            "Epoch [120/250], Loss: 0.1392\n",
            "Epoch [140/250], Loss: 0.1284\n",
            "Epoch [160/250], Loss: 0.1170\n",
            "Epoch [180/250], Loss: 0.1052\n",
            "Epoch [200/250], Loss: 0.0959\n",
            "Epoch [220/250], Loss: 0.0892\n",
            "Epoch [240/250], Loss: 0.0832\n",
            "Accuracy on test: 0.9100\n",
            "\n",
            "Training model - Epoch: 250, LR: 0.001, Batch_Size: 128\n",
            "Epoch [20/250], Loss: 0.5007\n",
            "Epoch [40/250], Loss: 0.3679\n",
            "Epoch [60/250], Loss: 0.2975\n",
            "Epoch [80/250], Loss: 0.2707\n",
            "Epoch [100/250], Loss: 0.2223\n",
            "Epoch [120/250], Loss: 0.2021\n",
            "Epoch [140/250], Loss: 0.1797\n",
            "Epoch [160/250], Loss: 0.1949\n",
            "Epoch [180/250], Loss: 0.1588\n",
            "Epoch [200/250], Loss: 0.1554\n",
            "Epoch [220/250], Loss: 0.1517\n",
            "Epoch [240/250], Loss: 0.1513\n",
            "Accuracy on test: 0.9050\n",
            "\n",
            "Training model - Epoch: 250, LR: 0.001, Batch_Size: 256\n",
            "Epoch [20/250], Loss: 0.6596\n",
            "Epoch [40/250], Loss: 0.5190\n",
            "Epoch [60/250], Loss: 0.4354\n",
            "Epoch [80/250], Loss: 0.3396\n",
            "Epoch [100/250], Loss: 0.3414\n",
            "Epoch [120/250], Loss: 0.2925\n",
            "Epoch [140/250], Loss: 0.2872\n",
            "Epoch [160/250], Loss: 0.2964\n",
            "Epoch [180/250], Loss: 0.2510\n",
            "Epoch [200/250], Loss: 0.2094\n",
            "Epoch [220/250], Loss: 0.2275\n",
            "Epoch [240/250], Loss: 0.2043\n",
            "Accuracy on test: 0.8750\n",
            "\n",
            "Training model - Epoch: 250, LR: 0.001, Batch_Size: 512\n",
            "Epoch [20/250], Loss: 0.6417\n",
            "Epoch [40/250], Loss: 0.5734\n",
            "Epoch [60/250], Loss: 0.4934\n",
            "Epoch [80/250], Loss: 0.4240\n",
            "Epoch [100/250], Loss: 0.3781\n",
            "Epoch [120/250], Loss: 0.3449\n",
            "Epoch [140/250], Loss: 0.3160\n",
            "Epoch [160/250], Loss: 0.2968\n",
            "Epoch [180/250], Loss: 0.2797\n",
            "Epoch [200/250], Loss: 0.2613\n",
            "Epoch [220/250], Loss: 0.2463\n",
            "Epoch [240/250], Loss: 0.2357\n",
            "Accuracy on test: 0.8550\n",
            "\n",
            "Training model - Epoch: 250, LR: 0.0001, Batch_Size: 16\n",
            "Epoch [20/250], Loss: 0.5521\n",
            "Epoch [40/250], Loss: 0.4367\n",
            "Epoch [60/250], Loss: 0.3657\n",
            "Epoch [80/250], Loss: 0.3242\n",
            "Epoch [100/250], Loss: 0.2958\n",
            "Epoch [120/250], Loss: 0.2743\n",
            "Epoch [140/250], Loss: 0.2577\n",
            "Epoch [160/250], Loss: 0.2438\n",
            "Epoch [180/250], Loss: 0.2320\n",
            "Epoch [200/250], Loss: 0.2216\n",
            "Epoch [220/250], Loss: 0.2121\n",
            "Epoch [240/250], Loss: 0.2034\n",
            "Accuracy on test: 0.8750\n",
            "\n",
            "Training model - Epoch: 250, LR: 0.0001, Batch_Size: 32\n",
            "Epoch [20/250], Loss: 0.6460\n",
            "Epoch [40/250], Loss: 0.5847\n",
            "Epoch [60/250], Loss: 0.5204\n",
            "Epoch [80/250], Loss: 0.4599\n",
            "Epoch [100/250], Loss: 0.4106\n",
            "Epoch [120/250], Loss: 0.3729\n",
            "Epoch [140/250], Loss: 0.3443\n",
            "Epoch [160/250], Loss: 0.3219\n",
            "Epoch [180/250], Loss: 0.3037\n",
            "Epoch [200/250], Loss: 0.2883\n",
            "Epoch [220/250], Loss: 0.2751\n",
            "Epoch [240/250], Loss: 0.2634\n",
            "Accuracy on test: 0.8600\n",
            "\n",
            "Training model - Epoch: 250, LR: 0.0001, Batch_Size: 64\n",
            "Epoch [20/250], Loss: 0.6497\n",
            "Epoch [40/250], Loss: 0.6115\n",
            "Epoch [60/250], Loss: 0.5728\n",
            "Epoch [80/250], Loss: 0.5308\n",
            "Epoch [100/250], Loss: 0.4931\n",
            "Epoch [120/250], Loss: 0.4576\n",
            "Epoch [140/250], Loss: 0.4328\n",
            "Epoch [160/250], Loss: 0.4068\n",
            "Epoch [180/250], Loss: 0.3868\n",
            "Epoch [200/250], Loss: 0.3646\n",
            "Epoch [220/250], Loss: 0.3531\n",
            "Epoch [240/250], Loss: 0.3440\n",
            "Accuracy on test: 0.8100\n",
            "\n",
            "Training model - Epoch: 250, LR: 0.0001, Batch_Size: 128\n",
            "Epoch [20/250], Loss: 0.7107\n",
            "Epoch [40/250], Loss: 0.6900\n",
            "Epoch [60/250], Loss: 0.6711\n",
            "Epoch [80/250], Loss: 0.6471\n",
            "Epoch [100/250], Loss: 0.6216\n",
            "Epoch [120/250], Loss: 0.6015\n",
            "Epoch [140/250], Loss: 0.5816\n",
            "Epoch [160/250], Loss: 0.5523\n",
            "Epoch [180/250], Loss: 0.5343\n",
            "Epoch [200/250], Loss: 0.5068\n",
            "Epoch [220/250], Loss: 0.4853\n",
            "Epoch [240/250], Loss: 0.4619\n",
            "Accuracy on test: 0.7650\n",
            "\n",
            "Training model - Epoch: 250, LR: 0.0001, Batch_Size: 256\n",
            "Epoch [20/250], Loss: 0.6416\n",
            "Epoch [40/250], Loss: 0.6441\n",
            "Epoch [60/250], Loss: 0.6194\n",
            "Epoch [80/250], Loss: 0.6115\n",
            "Epoch [100/250], Loss: 0.6004\n",
            "Epoch [120/250], Loss: 0.5724\n",
            "Epoch [140/250], Loss: 0.5725\n",
            "Epoch [160/250], Loss: 0.5511\n",
            "Epoch [180/250], Loss: 0.5389\n",
            "Epoch [200/250], Loss: 0.5165\n",
            "Epoch [220/250], Loss: 0.5111\n",
            "Epoch [240/250], Loss: 0.5088\n",
            "Accuracy on test: 0.7000\n",
            "\n",
            "Training model - Epoch: 250, LR: 0.0001, Batch_Size: 512\n",
            "Epoch [20/250], Loss: 0.6834\n",
            "Epoch [40/250], Loss: 0.6782\n",
            "Epoch [60/250], Loss: 0.6688\n",
            "Epoch [80/250], Loss: 0.6634\n",
            "Epoch [100/250], Loss: 0.6539\n",
            "Epoch [120/250], Loss: 0.6465\n",
            "Epoch [140/250], Loss: 0.6379\n",
            "Epoch [160/250], Loss: 0.6301\n",
            "Epoch [180/250], Loss: 0.6241\n",
            "Epoch [200/250], Loss: 0.6172\n",
            "Epoch [220/250], Loss: 0.6040\n",
            "Epoch [240/250], Loss: 0.5972\n",
            "Accuracy on test: 0.7400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Penjelasan Konseptual:\n",
        "# Setelah semua eksperimen selesai, kita punya hyperparam_results yang berisi tuple (epoch, lr, batch_size, acc).\n",
        "# Kita akan sortir berdasarkan akurasi untuk melihat konfigurasi mana yang paling baik.\n",
        "\n",
        "sorted_hyperparam_results = sorted(hyperparam_results, key=lambda x: x[3], reverse=True)\n",
        "\n",
        "print(\"Top 5 best configurations based on accuracy:\")\n",
        "for i in range(5):\n",
        "    ep, lr, bs, acc = sorted_hyperparam_results[i]\n",
        "    print(f\"Epoch: {ep}, LR: {lr}, Batch_Size: {bs}, Accuracy: {acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jH1JcNJfhUgr",
        "outputId": "32188f94-c054-4376-a279-28dffc8a97c9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 best configurations based on accuracy:\n",
            "Epoch: 50, LR: 0.1, Batch_Size: 64, Accuracy: 0.9250\n",
            "Epoch: 25, LR: 0.1, Batch_Size: 16, Accuracy: 0.9200\n",
            "Epoch: 100, LR: 0.1, Batch_Size: 32, Accuracy: 0.9200\n",
            "Epoch: 250, LR: 0.1, Batch_Size: 64, Accuracy: 0.9200\n",
            "Epoch: 50, LR: 0.01, Batch_Size: 16, Accuracy: 0.9150\n"
          ]
        }
      ]
    }
  ]
}